{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSearch Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前大模型官网的AI在做搜索的时候，由于服务器资源的限制，可能很多时候并不能完成我们特定领域的所有搜索任务。\n",
    "比如，我们要搜索数十篇论文并做分类，官网是不能很好地解决的，只会给我们几个结果。\n",
    "又或者我们想要爬取特定网站的数据，由于数据较为复杂，需要查看更多分页，它不能精确地解决我们的需求。\n",
    "\n",
    "这时候，我们可以做一个细化的搜索智能体，针对不同的需求，设计多类特定方向的爬虫，让大模型根据我们的问题，自主选择相应的爬虫工具，从而实现更好的搜索总结效果。\n",
    "\n",
    "这时候其实我们就是在做多智能体系统，用户的问题进来之后，会交给一个决策Agent，决定用哪个爬虫工具，然后爬取所需的信息之后，添加到上下文，根据用户的需求进行回答。\n",
    "\n",
    "在我的代码实现中，DeepSearchAgent有两个搜索工具：必应网页搜索和arXiv论文搜索。\n",
    "- 非论文搜索会直接使用必应搜索工具，比如一个问题：大模型领域最近有哪些新闻，这个问题决策Agent会分配给bing搜索工具，搜索出来多个链接然后提取链接中的所有内容添加到聊天中，让大模型拥有这些最新的信息，回答用户的问题。\n",
    "- 如果有论文搜索的需求，则会分配给arXiv论文搜索工具search_arxiv，搜索出来论文的标题、摘要、发布链接和PDF下载链接交给大模型回答用户的问题。\n",
    "\n",
    "每个工具都会有几个参数，比如搜索多少链接，多少论文等，这些都会根据我们的问题让大模型自主填写。\n",
    "当然，如果不需要搜索，直接聊天也是可以的~\n",
    "\n",
    "未来可以继续优化：\n",
    "- 增加知乎搜索、知网搜索等\n",
    "- 进行多关键词搜索-总结，然后深度分析-报告输出的功能，全自动解决问题\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_agent_tools.code_interpreter import * # 代码解释器\n",
    "from super_agent_tools.json_tool import * # json工具\n",
    "from super_agent_tools.llm_api import * # LLM模型API\n",
    "from super_agent_tools.llm_keys import * # LLM模型API Key\n",
    "from super_agent_tools.search_arxiv import * # arXiv搜索\n",
    "from super_agent_tools.search_bing import * # 必应搜索\n",
    "from super_agent_tools.code_interpreter import * # 代码解释器\n",
    "\n",
    "class DeepSearchAgent:\n",
    "    def __init__(self, model, zhipu_key):\n",
    "        self.model = model\n",
    "        self.temperature = 0.5\n",
    "        self.api_key = zhipu_key\n",
    "        # 判断用户需求的Prompt\n",
    "        self.judge_cmd_prompt = self.get_prompt(\"prompts/deepsearch/judge_cmd_prompt.txt\")\n",
    "        self.conversations = [{\"role\": \"system\", \"content\": \"你是一个搜索助手，根据你搜到的内容，回答我的问题。\"}]\n",
    "\n",
    "    def get_prompt(self, path):\n",
    "        with open(path, 'r', encoding=\"utf-8\") as file:\n",
    "            prompt = file.read()\n",
    "        return prompt\n",
    "\n",
    "    def get_pure_ans(self, question):\n",
    "        \"\"\"\n",
    "        纯粹回答，不增加至聊天记录\n",
    "        \"\"\"\n",
    "        ans = \"\"\n",
    "        for char in get_llm_answer(question, self.model, self.temperature):\n",
    "            ans += char\n",
    "            print(char, end=\"\", flush=True)\n",
    "        print(\"\\n\")\n",
    "        return ans\n",
    "    \n",
    "    def get_llm_ans_converse_yield(self):\n",
    "        \"\"\"\n",
    "        输入conversations, 输出回答（question不是参数）, 并将回答加入到对话中\n",
    "        生成器模式\n",
    "        \"\"\"\n",
    "        ans = \"\"\n",
    "        for char in get_llm_answer_converse(self.conversations, self.model, self.temperature):\n",
    "            ans += char\n",
    "            yield char\n",
    "        print(\"\\n\")\n",
    "        self.conversations.append({\"role\": \"assistant\", \"content\": ans})\n",
    "    \n",
    "    def get_llm_ans_converse_v2_yield(self, question):\n",
    "        \"\"\"\n",
    "        输入question, 输出回答, 并将回答加入到对话中\n",
    "        生成器模式\n",
    "        \"\"\"\n",
    "        self.conversations.append({\"role\": \"user\", \"content\": question})\n",
    "        ans = \"\"\n",
    "        for char in get_llm_answer_converse(self.conversations, self.model, self.temperature):\n",
    "            ans += char\n",
    "            yield char\n",
    "        print(\"\\n\")\n",
    "        self.conversations.append({\"role\": \"assistant\", \"content\": ans})\n",
    "    \n",
    "    # 判断用户的搜索属于什么类型\n",
    "    def judge_search_type(self, question):\n",
    "        prompt = self.judge_cmd_prompt.format(question=question)\n",
    "        judge = self.get_pure_ans(prompt)\n",
    "        judge = get_json(judge)[\"tool\"]\n",
    "        return judge\n",
    "\n",
    "    def answer(self, question):\n",
    "        tool = self.judge_search_type(question)\n",
    "        if tool != \"chat()\":\n",
    "            searched_texts = run_code(tool, globals=globals())\n",
    "            prompt = f'''\n",
    "我找到了以下内容：\n",
    "{searched_texts}\n",
    "'''\n",
    "            self.conversations.append({\"role\": \"user\", \"content\": question})\n",
    "            self.conversations.append({\"role\": \"assistant\", \"content\": prompt})\n",
    "            self.conversations.append({\"role\": \"user\", \"content\": \"回答我上述的问题\"})\n",
    "            for char in self.get_llm_ans_converse_yield():\n",
    "                yield char\n",
    "        else:\n",
    "            for char in self.get_llm_ans_converse_v2_yield(question):\n",
    "                yield char\n",
    "        \n",
    "    # 需要增加：知乎搜索、知网搜索\n",
    "    # 需要增加：持续搜索能力 - 切换关键词、增加更多搜索结果等，实现方式：增加一个关键词agent,记录历史搜索命令，让大模型可以根据历史搜索来更新新的搜索命令\n",
    "    # 需要增加：打开某一篇论文的详细内容的功能\n",
    "    # 需要增加：搜索-总结-深度分析-报告输出的功能，全自动解决问题\n",
    "    # 增加清空历史记录的功能\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"tool\": \"get_bing_searched_results(keyword='大模型 新闻', max_results=5)\"\n",
      "}\n",
      "```\n",
      "\n",
      "最近大模型领域有几个重要的新闻：\n",
      "\n",
      "1. **大模型年度榜单公布**：GPT-4在2023年度大模型评测榜单中位居第一，紧随其后的是智谱清言GLM-4、阿里巴巴Qwen-Max和百度文心一言4.0。这些模型在语言和知识等基础能力维度上与GPT-4 Turbo相当接近。然而，在复杂推理、数学、代码和智能体等方面，国内大模型与GPT-4相比还存在差距【7†来源】。\n",
      "\n",
      "2. **万亿MoE+多模态大模型矩阵亮相**：最近有报道提到，万亿MoE+多模态大模型矩阵已经亮相。这表明多模态大模型领域正在取得显著进展，可能会在未来几年内成为大模型技术的一个重要方向【8†来源】。\n",
      "\n",
      "3. **大模型落地难题**：大模型在行业应用中面临的主要问题包括算力不足、行业数据不足、应用经验不足、缺少专用芯片、数据处理难题、数据微调难题、思维模式转变难题、成本超高难题、运营模式改变难题和算力运营模式创新难题。这些问题需要通过技术创新和行业合作来解决【9†来源】。\n",
      "\n",
      "4. **国内大模型盘点**：截至2024年5月，国内共发布了305个大模型，但其中只有约140个完成了生成式人工智能服务备案，占比45.9%。这意味着还有约165个大模型尚未通过备案。大模型的发展面临着高昂的算力成本和商业化的挑战【10†来源】。\n",
      "\n",
      "这些新闻反映了大模型技术正在快速发展，同时也面临着一些关键的挑战和机遇。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-4o\"\n",
    "model = \"glm-4\"\n",
    "\n",
    "search_agent = DeepSearchAgent(model,zhipu_key=zhipu_key)\n",
    "question = \"帮我搜索10篇与大语言模型相关的论文，按照最新日期排序。\"\n",
    "question = \"OpenAI最近有什么新闻？\"\n",
    "question = \"大模型最近有哪些新闻\"\n",
    "\n",
    "ans = \"\"\n",
    "for char in search_agent.answer(question):\n",
    "    ans += char\n",
    "    print(char, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "我找到了以下内容：\n",
      "正在搜索 'https://cn.bing.com/search?q=大模型 新闻&first=1'...\n",
      "在第 1 页找到 10 个有效链接\n",
      "总字数：13854\n",
      "函数返回的结果:\n",
      "关于大模型 新闻的搜索结果如下：\n",
      "Link: https://www.thepaper.cn/newsDetail_forward_26209606\n",
      "登录 大模型年度榜单公布：GPT-4第一，智谱、阿里紧追 ·通过题海战术提高大模型成绩，对于模型实际能力的反应是失真的，影响了模型研发团队的改进方向和模型的商业落地，“高分低能”伤害的是机构本身。 ·国内大模型相比GPT-4还存在差距，推理、数学、代码、智能体是国内大模型短板，中文场景下国内最新大模型已展现出优势。 OpenCompass2.0大语言模型中英双语客观评测前十名（采用百分制）。商用闭源模型通过API形式测试，开源模型直接在模型权重上测试。 在一众试图“超越GPT”的大模型中，哪个大模型实力最强？大模型跑分、刷榜，如何测评大模型真实水平？ 1月30日，大模型开源开放评测体系司南（OpenCompass2.0）揭晓了2023年度大模型评测榜单。对过去一年来主流大模型全面评测诊断后，结果显示，GPT-4-Turbo在各项评测中均获最佳表现，国内厂商近期发布的模型紧随其后，包括智谱清言GLM-4、阿里巴巴Qwen-Max、百度文心一言4.0。 评测是大模型的指挥棒和指南针，OpenCompass为模型提供评测服务，量化模型在知识、语言、理解、推理和考试等五大能力维度的表现。总体来看，大语言模型整体能力仍有较大提升空间，复杂推理相关能力仍是大模型普遍面临的难题，国内大模型相比于GPT-4还存在差距。中文场景下国内最新大模型已展现出优势，在部分维度上接近GPT-4-Turbo的水平。 中英双语客观评测：数学、代码仍是短板 OpenCompass于2023年7月由上海人工智能实验室在世界人工智能大会上推出，目前升级为OpenCompass2.0，构造了一套中英文双语评测基准，涵盖语言与理解、常识与逻辑推理、数学计算与应用、多编程语言代码能力、智能体、创作与对话等方面。 基于语言、知识、推理、数学、代码、智能体等六个维度，OpenCompass2.0构建了超1.5万道高质量中英文双语问题，并引入首创的循环评估（Circular Evalution）策略，系统分析了国内外大模型的综合客观性能。 中英双语客观评测榜单。截至该榜单发布，部分新大模型尚未纳入本次评测。 在百分制的客观评测基准中，GPT-4 Turbo仅达到61.8分的及格水平。此结果显示，复杂推理仍然是大模型面临的重要难题，需要进一步的技术创新来攻克。 在综合性客观评测中，智谱清言GLM-4、阿里巴巴Qwen-Max和百度文心一言4.0具有较为均衡和全面的性能，这些模型在语言和知识等基础能力维度上可比肩GPT-4 Turbo。 推理、数学、代码、智能体是国内大模型的短板。GPT-4 Turbo在涉及复杂推理的场景虽然也有提升空间，但已明显领先于国内的商业模型和开源模型。国内大模型要整体赶超GPT-4 Turbo等国际顶尖的大模型，在复杂推理、可靠地解决复杂问题等方面仍需下大功夫。 中文主观评测：闭源模型接近GPT-4 基于语言、知识、创作、数学与推理等五个维度，OpenCompass2.0构建了超500道高质量中文问题，采用基于大语言模型对战的方式评测主流模型在开放场景下的对话体验。 中文主观评测。截至该榜单发布，部分新大模型尚未纳入本次评测。 “主观评测中，最难的是主观的数学题，不能靠猜答案。”上海人工智能实验室领军科学家林达华表示，榜单中，GPT-4 Turbo的数学能力遥遥领先，说明在高难度的推理上具有优势。 基于主观评测分析，研究人员还发现，国内商用大模型在中文评测中表现优秀，和GPT-4 Turbo差距缩小。阿里巴巴Qwen-Max、智谱清言GLM-4、百度文心4.0都取得了优秀成绩。在中文语言理解、中文知识和中文创作上，国内商业模型相比GPT-4 Turbo有更强的竞争力。 开源社区的Yi-34B-Chat、InternLM2-Chat-20B在综合性对话体验上表现突出，它们以中轻量级的参数量、接近商业闭源模型的性能，为学术研究和工业应用提供了良好基础。国内开源模型近期快速进步展现了开源模型的应用潜力，开源模型和开源工具体系的结合可帮助企业快速试验大模型在应用场景的适用性。 目前OpenCompass2.0已和合作伙伴共同推出了多个垂直领域的评测基准和数据集，包括LawBench法律大模型评测基准、OpenFinData金融评测集、MedBench医疗大模型评测系统、SecBench网络安全大模型评测平台等。 吸取高考经验，避免大模型直接刷题 “评测是大模型的指挥棒和指南针。”林达华教授表示，大模型评测要客观公允、评测方式科学、评测维度全面。OpenCompass2.0的评测维度包括基础能力和综合能力两个层级，能力维度设计具备可扩展性和增长性，同时可根据未来的大模型应用场景进行动态更新和迭代。 基础能力维度以语言、知识、理解、数学、代码为核心，包括意图识别、情感分析、内容评价与总结、多语言翻译、汉语与中国传统文化、常识百科、自然科学、人文社科、计算能力、数学应用能力、多编程语言代码等20余项细分任务。而综合能力旨在考察模型在综合运用知识、数学推理、代码工具等多种能力完成复杂任务的水平。 当前，一些大模型沉迷于刷榜、跑分。林达华表示，通过题海战术提高大模型成绩，对于模型实际能力的反应是失真的，影响了模型研发团队的改进方向和模型的商业落地，“高分低能”伤害的是机构本身。为此，实验室吸取了高考经验，提前公布“考试大纲”，但在第一期测评榜单发布前不公开“考题”，下一期“考题”用于下一期测评，每一期题目不同，避免大模型直接刷题，从而发现能力长板与短板。未来也会考虑开发测评分集，对于高分考生，用更有挑战、区分度更大的题目进行测评，凸显能力差距。 “国内有很多模型正在发布的路上，榜单上任何具体的名字只是大模型成长过程中无数次测试中的一次，一时的排名高低并不真正反映模型的能力，最重要的是每一次测验可以回过头来指导我们改进自己。”林达华表示。 Android版 iPhone版 iPad版 沪ICP备14003370号 沪公网安备31010602000299号 互联网新闻信息服务许可证：31120170006 增值电信业务经营许可证：沪B2-2017116 © 2014-2024上海东方报业有限公司\n",
      "==================================================\n",
      "Link: https://new.qq.com/rain/a/20240704A061UK00\n",
      "揭秘：阶跃星辰万亿MoE+多模态大模型矩阵亮相 机器之心 2024-07-04 16:20发布于北京机器之心官方账号\n",
      "==================================================\n",
      "Link: https://www.thepaper.cn/newsDetail_forward_25835029\n",
      "登录 年度话题：大模型落地的十大难题 作者 | 山竹 出品 | 锌产业（公众号：xinchanye2021） 2023年12月13日，全球科技顶刊《Nature》发布年度十大人物，与以往不同的是，今年的Nature’s 10额外增加了一个非人类，ChatGPT。 2023年12月20日，国家语言资源监测与研究中心、商务印书馆等单位联合主办的“汉语盘点2023”公布的年度国际词，同样是ChatGPT。 ChatGPT让大模型席卷全球，大模型被全球科技领袖定义为一次颠覆式科技革命。 微软CEO Satya Nadella说，“AI是我正在经历的第五次重大变革。” 腾讯CEO马化腾说，“AI是几百年不遇、类似发明电的工业革命一样的机遇。” 英伟达CEO黄仁勋更是大胆预测，“两年之内，英伟达乃至整个行业都会面目全非。” 我们不是先知，无法预见未来，但全球这些最具影响力、最有资源的企业领袖一致警觉和大手笔投入，必然会在短期内形成一个不可逆的产业趋势。 因此，无论能否对社会带来颠覆性改变，或者能够带来多大的改变，大模型都值得每个人分出一部分精力去了解、关注。 12月24日，在中国信通院人工智能论坛上，智源研究院大模型行业应用负责人周华、中国移动研究院AI中心副总经理金镝、华为云产业发展高级专家翟传璞、硅动科技CEO袁俊辉、科大讯飞北京研究院院长助理李家琦、国网智能电网研究院计算及应用研究所副所长石聪聪就大模型落地难题进行了一场对话。 值得注意的是，这些人是大模型时代国内最先觉醒的那批亲历者、实践者、创业者，也是国内大模型研究、应用、创业者中最典型的代表。 周华说，现在的大模型还是一个“文科生”； 袁进辉说，大模型亟需访存密集型专用芯片； 金镝说，从「小模型」到「大模型」思路难转变； 翟传璞说，算力也应该有如云服务一样的租赁模式； 石聪聪说，大模型在行业中还没有一个很成熟的案例； 李家琦讲述了科大讯飞科技文献行业大模型落地过程中从中科院文献中心的“借兵”经历； …… 他们这一年的亲身经历与所思所想，是中国大模型产业发展最好的沉淀。 从中，锌产业总结了大模型落地的十​大难题，它们分别是： 算力不足、行业数据不足、应用经验不足、缺少专用芯片、数据处理难题、数据微调难题、思维模式转变难题、成本超高难题、运营模式改变难题、算力运营模式创新难题。 以下是锌产业特别就这场对话中部分关键内容进行的不改变原意的整理，借几位亲历者的亲身经验和真知灼见，一起深入了解当下大模型的魔力边界： 01 大模型落地的十个难题 问：这一年，大模型在落地过程中，遇到了哪些问题。 石聪聪：行业大模型在应用上有四方面挑战： 第一，大模型的行业知识不足。 我们对比了现在很多通用大模型，行业知识、语料不足，很难解决行业中遇到的复杂任务，所以通用大模型用于行业，需要做二次预训练。 第二，算力不足。 因为需要做二次预训练，对行业算力需求很迫切。尤其是在如今形势下，我们对国产算力需求尤为迫切，我们现在算力大概有几百P，甚至上千P的缺口。 第三，数据样本不足。 因为我们需要准备大量行业语料（需要几百B，甚至几百T），涉及到各个业务领域，尤其很多数据还涉及用户敏感信息，这些数据的融合、脱敏也有一定难度。 第四，应用经验不足。 传统小模型也能解决很多问题，现在大模型能否一统天下，还需要观察。 近几年，我们还是需要考虑大模型和小模型如何协同应用，这也是需要我们重点关注的问题。 李家琦：我简单谈一下我们大模型在科技文献领域落地遇到的问题。 我们当时主要遇到了两方面难题： 第一，数据处理难题。 我们当时从中国科学院文献情报中心拿到了千万级的PDF论文数据，这个量级的论文解析很困难。 如何对这些数据进行高质量清洗，并形成足够多数据对大模型进行二次训练，这个是比较有难度的。 我们当时使用了很多开源PDF解析工具，最后用我们自己的OCR解析软件才解决了这些问题。 第二，数据微调（SFD）难题。 因为数据构造时，例如面向一篇生物论文构造数据时，这篇论文的创新性是什么？肯定需要生物领域的专家才能给出一个较好的判断。 所以我们最后是从中科院文献中心协调了二三十人来帮助我们进行数据标注，这样才完成了数据标注任务。 此外，在大模型研发过程中，我们完全使用了华为昇腾910B。 早期，我们也是刚开始使用国产硬件进行大模型落地开发，在算子适配上遇到了一些问题，后来在华为的帮助下，解决了算子适配问题。 最终，我们从4月到10月，用了半年时间，完成了大模型在科技文献领域的行业落地。 袁进辉：我们认为大模型推理、部署的成本未来会成为一个主要问题。 我们都相信大模型未来会无处在，要想无处不自在，今天来看成本还是比较贵的，这个已经有很多证据。 国内现在大模型使用量还没有那么高，海外有的应用使用量很高，已经暴露出这个问题。 例如，微软GPT写代码的助手付费用户超过100万时，虽然每个月每位用户会交20美元服务费，但实际上微软还要亏几十美元。这就说明用户付的费用还是cover不了它的成本。 OpenAI前段时间发布GPTs时，用户量突然暴增，OpenAI因此停止注册了一段时间，因为它只有那么几万块GPU在工作，如果再增加用户，就会影响之前用户的使用体验。 这些问题都暴露出来，今天大模型推理成本还是太高了。 几年前，我们在手机上下载张图片、下载个视频，都要精打细算，要考虑这个月流量有多少。但今天我们在手机、微信上刷视频，其实不会再考虑成本的问题。 大模型要真正做到无处不在的话，一定要像今天我们使用带宽一样“不心疼”。 翟传璞：大模型要发展，无非就是算力、算法、数据几个层面。 从算力层面来看，我们认为应该有多种供给方式。 一种是单独通过算力购买的方式，另外，我们也在思考，我们能否采用一种新的方式——类似云服务租赁来租赁算力方式提供算力。 这样一来，我不仅能满足短时间大规模算力的需求，国产算力迁移和适配能力也可以考虑通过工具——在云服务上提供迁移、优化工具来实现。 从算法和模型层面来看，大模型与行业结合很重要。 华为大模型的重点是在To B领域，To B领域和行业经验结合非常重要。 例如我们在《Nature》上发表的气象大模型，仅仅靠算法工程师是完不成的，它一定是算法工程师和气象学专家一起努力才能完成。 从数据层面来看，大家都说数据获取、数据标注比较困难，我们是希望把我们内部这种算法使用、标注的能力贡献出来。 另外，我们希望有一些技术能够解决数据流通的问题。 例如现在欧洲喊得比较多的可信数据空间、可信数据交换的能力，能不能应用在AI数据获取和流通环节，将有限数据发挥出更大价值。 金镝：中国移动是从今年年初开始启动大模型研发工作的。 现在我们也推出了139亿参数的语言大模型，推出之后，我们在公司内部和客户中加快推进大模型落地。 在这个过程中，我也有一些自己的体会。 第一，现在面临较大挑战是，行业如何看待和拥抱大模型。 我们在集团内部，包括见一些行业客户，他们都会问我们，大模型到底能干什么？能带来什么价值？要先从哪些领域开始用？ 这些都是很现实的问题。 这其中涉及一种思维方式的转变。包括我经常也会把思维限定在原来信息化和小模型思维体系中。在提解决方案时，想着想着又变成了IT化的竖井模式，变成了原来的「+AI」模式。 现在很多时候，用大模型去替代原来的小模型只是一种改良，没有把大模型真正的价值激发出来。 所以包括我们自己、我们的用户，都要思考到底想用大模型干什么？希望它能带来什么变化？ 第二，大模型运营模式需要变化。 我们已经把大模型用在了中国移动客服领域、网络运维领域，用上后发现，它虽然提升了业务体验，但也需要企业改变后端业务运营方式。 例如，大模型里的数据是有时效性的，假如在服务客户时，我们发现了一个需要马上解决的问题，解决这个问题是用打补丁方式，还是基于大模型方式来快速响应，这是摆在我们面前一个现实的问题。 第三，成本问题。 大家都觉得大模型非常好，但一说到用大模型，需要买多少算力，投多少人做数据治理、训练模型，需要多少人做运维，用户就不敢用了，这是行业客户一个非常现实的问题。 周华：我今年接触了很多行业客户，一般来说，我们和行业客户沟通会先问两个问题： 第一，你的数据怎么样；第二，你有多少算力。 数据层面，大模型应用也就一年，时间并不长，很多行业客户对数据认识并不深刻。 那这其中有什么问题呢？ 主要问题是，我们有大量客户对自身数据能不能用于大模型训练并不清楚，很多时候，大家一说数据，都是数据库里的数据，或者大数据平台中的数据。 其实这些数据是无法用于模型训练的。 另外，很多客户平时并不会积累行业相关的文本数据，例如领域里的论文、教科书等，但是这些恰恰对行业大模型训练来说非常重要。 客户自身的数据，有些放在数据库里，有些以PDF等文件形式存放在不同地方，要用这些未经整理的数据训练模型，成本往往非常高。 所以，我们往往都会建议客户，首先数据处理要有专人负责，要做大模型首先要把数据做好，甚至在规划大模型过程中，就要做好数据整理。 在行业层面，我们更推崇多家企业共同推动一个行业模型训练的模式。 行业模型的数据每家都去做的话，成本非常高，也很浪费资源,这项工作很适合通过行业协会来做。 算力层面，我们很多客户没有A100、H100、A800，这些企业手上一般是有消费级显卡，3090、4090，如果要做模型训练肯定存在很多问题。 我们研究院最近针对这一问题，在做很多研发工作，包括4比特量化、DPU等相关技术。 我们会和客户深入沟通，希望这些客户对低资源的模型训练可以有一定认识，这样训练出的模型能否满足他们的需求要有一定的认识。 这样，在大模型落地过程中就可以节省大量资源。 02 预见2024：大模型的八个变化 问：2024年，大模型会在哪些新应用场景爆发。 周华：我们做大模型的研发看到了一些趋势，这里我谈两点： 第一，多模态大模型会有很大的突破。 我们看到最近谷歌发布了Gemini，这之前，OpenAI发布了GPT-4V，在开源领域、在学术界，也有大量像LLaMA模型出来，这让业界对多模态大模型产生了研发兴趣。 我们现在和制造业有很多对接，他们对于工程制图的解析、几何体的解析是有需求的，但我们现在最好的GPT-4都做不好。 在多模态这块，现在的发展还很初步，我觉得2024年，会有很多团队来解决这些问题。 第二，现在的大模型还是一个比较强的“文科生”，未来会在行业场景有突破。 它可以做一些文字处理，也会解决一些专业问题，但专业度并不高，工业制造这种场景有大量对准确性、专业性有很高要求，有很多很深入的事实性问题，现在的大模型依然存在幻觉率。 我们研究院现在在致力于做这些事，希望把我们大模型的逻辑能力、数学、物理能力提升，未来能够在制造业使用。 金镝：我们认为现在人工智能已经在从「+AI」向「AI+」这个方向发展了。 结合我自己的工作体会，我觉得： 第一，在能够充分发挥大模型认知、理解、生成的场景下（如辅助办公），短期内会有很大的变化。 我自己身边就有这样的例子，我们UX设计师现在已经在大量使用大模型做设计了，这极大地提升了他自己的工作效率。 我们研究院在OA系统中，现在也在嵌入一些辅助文档，用大模型给大家提供辅助办公帮助，这个场景在2024年一定会有很大的变化。 第二，我们也会致力于解决行业大模型的幻觉问题。 我们接触的很多客户，例如医疗、政府、运营商，他们要求大模型不能胡说八道，大模型给出的答案一定是百分之百准确的，或有极高准确率。这样才能完成专业工作。 第三，我们看到了端云协同的趋势。 特别是手机端的芯片、PC端的芯片最近一波发布，极大地增强了NPU能力，端侧大模型和云端大模型结合起来后，一定会诞生出很多新型智能终端型产品。 翟传璞：从我看，有两个方面： 第一，大模型会从2023年的单点尝试，向2024年小规模复制落地发展。 例如政务、金融、气象等领域，我们做了台风预测大模型后，现在泰国气象局也在复制落地。 第二，Gartner认为大模型未来场景选择会遵从“4C理论”。 第一个C是技术成熟度，第二个C是场景商用化， 第三个C是紧迫性，第四个C是成本。 我们去考虑未来场景，可以从这四个方面着眼，来判断大模型能在哪些场景率先落地。 袁俊辉：我蛮期待专门针对大模型推理需求芯片的出现。 客观来说，今天市场上所有GPU和芯片，都是在大模型出现之前、为之前负载设计的。 从技术上来看，之前的任务很多是计算密集型，计算为主，所以这些芯片通常会堆很多计算单元。 但在大模型出现之后，大家意识到，大模型推理是一个访存密集的事儿。所以很多芯片在今天都不是特别适用于大模型推理。 但是现在芯片已经改不了了，新架构没出现时，就只能通过软件来打补丁。 可能之后会出现专门针对大模型特性，例如针对访存瓶颈的访存密集型芯片架构出现。 李家琦：我非常同意袁老师的看法，我们其实非常需要专门面向推理的芯片，来降低部署大模型的成本。 在软件层面，目前主要是从模型压缩、量化角度来将计算量大幅降低。 在产业层面，现在大模型落地有很多推理方面的需求，我觉得未来会诞生一些软件平台，专门提供推理服务。 石聪聪：2023年面向个人用户的话，大模型用的场景还是很多的，包括人机交互、辅助办公。 对于行业用户，我们看到行业大模型在金融、政务、能源、教育领域已经有一些应用。 但是说实话，我们也确实没有看到大模型在行业里有一个非常成熟的案例。 从我们行业来说，我们也在积极拥抱大模型。 明年我们国网公司会拿出很多场景，包括客服、无人机巡检，甚至调度运行，我们都会拿出来探索大模型的应用。 本文为澎湃号作者或机构在澎湃新闻上传并发布，仅代表该作者或机构观点，不代表澎湃新闻的观点或立场，澎湃新闻仅提供信息发布平台。申请澎湃号请用电脑访问http://renzheng.thepaper.cn。 Android版 iPhone版 iPad版 沪ICP备14003370号 沪公网安备31010602000299号 互联网新闻信息服务许可证：31120170006 增值电信业务经营许可证：沪B2-2017116 © 2014-2024上海东方报业有限公司\n",
      "==================================================\n",
      "Link: https://www.nbd.com.cn/articles/2024-05-17/3393020.html\n",
      "每经网首页>要闻> 正文 “百模大战”周年考|国内大模型盘点：305个大模型发布，备案率约四成，如何寻找变现、破局之路 每日经济新闻2024-05-17 16:34:12 ◎ 截至5月16日，国内发布的305个大模型中约有六成尚未成功备案。一名大模型行业创业者告诉每经记者，没备案的大模型也不代表就消失在市面上。对于大模型的商业化，有行业人士认为，当下和未来两三年，大模型的商业探索会在成本和Token质量上相互妥协，并逐渐分化为两派。 每经记者 可杨 杨卉 每经编辑 兰素英 【编者按】： 本周，OpenAI推出新一代旗舰AI模型——GPT-4o。而早在2023年3月15日，GPT-4就已正式问世，其强大的文本生成能力迅速使生成式AI成为全球焦点，掀起了一场AI技术竞赛的浪潮。 在国内，生成式大模型的发布同样风起云涌。2023年3月16日，百度发布“文心一言”大模型；2023年4月10日，商汤科技的日日新发布；2023年4月11日，阿里巴巴的通义千问发布；2023年7月7日，华为云推出盘古大模型3.0……各方力量争先恐后，争奇斗艳，这股热潮被形象地称为“百模大战”。 那么，一年多过去了，国内大模型企业的发展现状如何？硅谷的生态又有怎样的新变化？在这一领域中，科技巨头和初创企业展现出了怎样的发展方向？对此，《每日经济新闻》特推出《“百模大战”周年考》策划，深入探讨这些问题。 一年前的3月15日，随着OpenAI多模态预训练大模型GPT-4的发布，国内包括百度、华为、腾讯等科技巨头，百川智能等初创企业，以及智谱AI研究院等研究机构纷纷扬帆起航，投身到人工智能（AI）大模型开发，试图搭上这趟时代的列车，轰轰烈烈的“百模大战”也由此开启。 据《每日经济新闻》记者的不完全统计，截至今年4月底，国内共计推出了305个大模型。而截至5月16日，只有约140个大模型完成生成式人工智能服务备案，占发布总量的45.9%。这意味着，还有约165个大模型尚未获得“过审”机会。 这一严峻现实的背后除了有技术层面的难度，还有训练和推理过程中高昂算力成本的制约；即便是跨过这一关，大模型企业如何实现商业化，依然着面临不小的难度。而对这场竞赛中可能被“出局”的公司来说，未来的路又在何方呢？ 现状：305个大模型发布，仅约四成完成备案 GPT-4的发布在全球掀起了“炼大模型”的热潮，面对这一新蓝海，科技巨头、初创企业以及科研院校相继开启布局，没人想错过这趟时代的列车。 据《每日经济新闻》不完全统计，截至今年4月底，国内共推出了约305个大模型，在过去一年推动着语言理解、图像识别等多个领域的技术进步。 截至2024年5月16日，国内共有约140个大模型完成生成式人工智能服务备案，占305个大模型的45.9%左右。 此前，国家网信办有关负责人就《办法》相关问题回答媒体提问时介绍，《办法》规定，利用生成式人工智能技术向中华人民共和国境内公众提供生成文本、图片、音频、视频等内容的服务，适用本办法。 在已备案的大模型中，在地域分布上，北京以70个备案大模型领跑全国，凸显了其在AI领域的集聚效应。上海和广东紧随其后，分别有28个和19个模型备案。 而“140”这一数字同时也意味着，从备案层面来看，大约还有165个大模型依旧未通过备案，无法公开向公众提供服务。这些尚未能“过审”的大模型中，不乏一些备受关注的明星项目，包括曾号称是“国内首个ChatGPT”的元语智能大模型ChatYuan。 更多未完成备案的是“学院派”大模型。在305个大模型中，有60个大模型是由大学或研究院所研发。或许是由于研究机构的项目更偏重学术探索，而非商业应用，备案动力或流程可能不如企业迅速。也有大模型转向“境内深度合成服务算法”备案，例如恒生电子的大模型。 一名大模型行业创业者对《每日经济新闻》记者介绍称，当前模型相关的备案申请有点像专利申请，并不一定会通过，且申请周期较长，约在4~6个月。他表示，当下，大模型只要做To C服务，就需要备案，而在B端，一些大客户会要求大模型公司完成备案工作。 不过他同时强调，没备案的大模型也不代表就消失在市面上，很多来自研究所、大学的大模型仅仅只用于研究，就没有动机去完成备案。 一家大模型头部企业从业人士也告诉记者，来自大学的大模型，如果只做自己学术范围内的研究，是可以不用备案的。 “百模大战”行至此时，最终留下3~5家大模型已经成为行业对于这场竞赛最终结局的共识。“大模型这个行业（到最后）可能就不存在了，未来大模型就是几个最基本的底座，只有少数的几家公司。”行行AI董事长、顺福资本创始人李明顺曾在接受《每日经济新闻》记者采访时坦言。 难点：成本高，日活千万需年入超100亿元才能覆盖数据中心成本 算力资源的稀缺性是制约大模型发展的关键瓶颈。对不少大模型来说，没能挺过一周年，难搞的算力要负很大责任。对于模型厂商而言，目前主要的算力成本包括预训练成本和推理成本。模型推理应用阶段对算力的需求要远远高于训练阶段。 据中国工程院院士郑纬民计算，在大模型训练的过程中，70%的开销要花在算力上；推理过程中95%的花费也是在算力上。 以GPT-4为例，该模型的训练需要一万块英伟达A100芯片跑上11个月。假设每块A100的成本为10000美元（价格因供应商和购买数量而异），那么一万块A100的总成本约为1亿美元。 对于许多急匆匆踏上大模型赛道的创业公司或科技企业来说，在“烧”了一阵子钱后，他们绝望地发现，算力不仅越来越贵，质量也开始下降。 郑纬民表示，目前，市面上只有三类系统可支持大模型训练。其中，基于英伟达GPU的系统一卡难求；基于国产AI芯片的系统面临国产卡应用不足、生态系统有待改善的问题；而基于超级计算机的系统，虽然可在作好软硬件协同设计的情况下实现大模型训练，但需在超算机器尚未饱和的前提下操作，私人企业获得超算设备的机会并不大。 据英特尔方面介绍，在大模型领域，去年关注点更多是在模型训练上，对成本和功耗并不那么重视，彼时，企业都希望能训练一个自己的通用大模型。随着很多通用大模型被训练出来，今年关注的重点则转移到了推理。对企业来说，大模型训练出来是需要变现且能够盈利的。但目前市场上很多大模型都是基于开源的，性能差不多，用于训练的数据也差不多，很难通过差异化来盈利。 没有足够的资金支撑推理过程，成了很多创业者败退的原因之一。为了降低成本，部分企业正在尝试探索是否可以用CPU来做大模型推理。从当前一些案例来看，在130亿参数以下的大模型中，CPU是可以做到的这一点的。 然而，即便是熬过了推理关，企业要将大模型变现仍有不小的难度。在行云集成电路创始人季宇看来，大模型的商业落地与早期互联网时代相比区别很大，边际成本仍然非常高。大模型每增加一个用户，基础设施需增加的成本是肉眼可见的，一个月几十美元的订阅费用根本不足以抵消背后高昂的成本。 更为关键的是，眼下大模型要大规模商业化，在模型质量、上下文长度等方面还有进一步诉求，不排除会进一步增加边际成本。目前来看，日活千万的通用大模型一年需超过100亿元的收入才能支撑其背后的数据中心成本，未来大模型要像互联网产业一样服务上亿人，成本一定是迈不过去的槛。 寻找新“航道”：投身应用或专注垂类细分行业 如果说“百模大战”最后的赢家只属于少数几家公司，那在这场赛事中被淘汰的公司，未来会走向何方？ 昆仑万维董事长方汉曾在接受《每日经济新闻》记者采访时表示，“百模大战”会淘汰一部分公司，剩下的科技公司肯定会继续全速前进。 在行云集成电路创始人季宇看来，当下和未来两三年，大模型的商业探索会在成本和Token质量上相互妥协，并逐渐分化为两派。 一派是质量优先，用高端系统打造高质量的通用大模型，寻找超级应用来覆盖高昂的成本。另一派是成本优先，用足够便宜的硬件提供基本够用的Token质量，寻找垂直场景的落地。若能在同样的成本下买到规格大得多的芯片，跑一个百亿千亿模型，支持超长上下文，商业化的空间会比今天大得多，就像曾经的显卡和游戏行业一样。 启明创投合伙人周志峰认为，当下，绝大多数的大模型企业是包着大模型的皮，裹着应用的心，“拥有模型能力的团队更容易在算法、模型、数据、模型的加速方面去做优化，以达到体验更好的产品，尤其对比那些用第三方模型纯粹做应用的公司。这一类公司其实不是模型公司，未来一定会是一家应用公司”。 周志峰以字节跳动为例，从今日头条到抖音到TikTok，背后的轴是AI驱动的推荐引擎。“字节跳动第一轮、第二轮融资的时候跟我们投资人讲得更多的故事是AI驱动的推进引擎，而今天不会再去说字节跳动是一家AI技术公司，只会记得是哪几个应用造成了这么大的规模。”同理，今天大部分的大模型公司未来也一定是靠它最终闯出了超级应用，大家因为这个超级应用而记住这家公司。 李明顺也持同样的观点，即不远的未来，有一部分大模型公司要转型成应用公司，因为大模型领域不需要这么多公司，“有一些大模型公司的创始人有Plan A和Plan B的双计划，就是一旦我的模型实在是拼不过前面的5家之后，就要被迫在一些垂直领域里面找到生存之地，它就会转型为一家应用公司。” 在备案成功的大模型中，部分模型已经从通用型转变为聚焦特定领域或行业的细分垂类模型。 中科闻歌董事长王磊在接受《每日经济新闻》记者采访时坦言，在过去的半年到一年内，适当做小行业大模型，降低参数规模的趋势已经变得非常明显。真正成功的商业应用不是制造一个巨无霸，而是能够被用户广泛使用且价格适中。“实用至上是关键，不必为了面子而去追求大规模，高昂的代价会影响产品的市场推广和用户的使用，实用性才是商业发展的主导原则。” 王磊表示，目前国内企业都意识到，最受欢迎的规模是70亿和130亿，300亿是单台推理的参数规模，比较受欢迎。“在我们的大模型发布时，国外网友评价这是企业级应用的小型参数规格。我认为一般的企业可能难以承受更大规模的产品。对于文本生成任务，这个规模基本上是足够的，但对于一些特定领域的任务，还需要强化模型的能力。” 第四范式也同样坚定选择投入行业大模型。“如果说无限把模型做大，往里面放无限多的数据，最后可能会达到AGI的状态，但是在每一个垂直应用，我们都要平衡好能力以及代价”。创始人戴文渊此前在第四范式的业绩沟通会上也表示，从技术的角度来说，第四范式也追求AGI，但是与此同时，“对于每一个客户的具体场景，我们也要做一定的裁剪，比如说这个考试只考数学，不一定需要让它有物理的能力。” 如需转载请与《每日经济新闻》报社联系。未经《每日经济新闻》报社授权，严禁转载或镜像，违者必究。 读者热线：4008890008 特别提醒：如果我们使用了您的图片，请作者与本站联系索取稿酬。如您不希望作品出现在本站，可联系我们要求撤下您的作品。 上一篇文章 在岸人民币兑美元收盘报7.2242，较上一交易日下跌54点 下一篇文章 新奥股份：部分董事及高管拟合计减持不超92.42万股 欢迎关注每日经济新闻APP 0 0 Copyright © 2024 每日经济新闻报社版权所有，未经许可不得转载使用，违者必究。 广告热线 北京: 010-57613265， 上海: 021-61283008， 广州: 020-84201861， 深圳: 0755-83520159， 成都: 028-86512112 互联网新闻信息服务许可证：51120190017网站备案号：蜀ICP备19004508号-3川公网安备 51019002002026号 新闻职业道德监督热线：400 889 0008 邮箱：zbb@nbd.com.cn\n",
      "==================================================\n",
      "Link: https://new.qq.com/rain/a/20230424A04SJH00\n",
      "大模型横行：不到2月10余个问世，烧30亿就能炼造？激战背后机会在哪儿？ 每日经济新闻 2023-04-24 14:43发布于四川每日经济新闻官方账号\n",
      "==================================================\n",
      "\n",
      "搜索链接：['https://www.thepaper.cn/newsDetail_forward_26209606', 'https://new.qq.com/rain/a/20240704A061UK00', 'https://www.thepaper.cn/newsDetail_forward_25835029', 'https://www.nbd.com.cn/articles/2024-05-17/3393020.html', 'https://new.qq.com/rain/a/20230424A04SJH00']\n",
      "代码执行成功\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(search_agent.conversations[2].get(\"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在搜索 'https://cn.bing.com/search?q=大模型&first=1'...\n",
      "在第 1 页找到 10 个有效链接\n",
      "Error fetching https://www.aigc.cn/large-models: \n",
      "总字数：29218\n",
      "关于大模型的搜索结果如下：\n",
      "Link: https://blog.csdn.net/leah126/article/details/139140426\n",
      "什么是大模型？一文读懂大模型的基本概念（非常详细）零基础入门到精通，收藏这一篇就够了 **大模型是指具有大规模参数和复杂计算结构的机器学习模型。**本文从大模型的基本概念出发，对大模型领域容易混淆的相关概念进行区分，并就大模型的发展历程、特点和分类、泛化与微调进行了详细解读，供大家在了解大模型基本知识的过程中起到一定参考作用。 本文目录如下： 1.大模型的定义 **大模型是指具有大规模参数和复杂计算结构的机器学习模型。**这些模型通常由深度神经网络构建而成，拥有数十亿甚至数千亿个参数。大模型的设计目的是为了提高模型的表达能力和预测性能，能够处理更加复杂的任务和数据。大模型在各种领域都有广泛的应用，包括自然语言处理、计算机视觉、语音识别和推荐系统等。大模型通过训练海量数据来学习复杂的模式和特征，具有更强大的泛化能力，可以对未见过的数据做出准确的预测。 ChatGPT 对大模型的解释更为通俗易懂，也更体现出类似人类的归纳和思考能力：大模型本质上是一个使用海量数据训练而成的深度神经网络模型，其巨大的数据和参数规模，实现了智能的涌现，展现出类似人类的智能。 那么，大模型和小模型有什么区别？ 小模型通常指参数较少、层数较浅的模型，它们具有轻量级、高效率、易于部署等优点，适用于数据量较小、计算资源有限的场景，例如移动端应用、嵌入式设备、物联网等。 而当模型的训练数据和参数不断扩大，直到达到一定的临界规模后，其表现出了一些未能预测的、更复杂的能力和特性，模型能够从原始训练数据中自动学习并发现新的、更高层次的特征和模式，这种能力被称为“涌现能力”。而具备涌现能力的机器学习模型就被认为是独立意义上的大模型，这也是其和小模型最大意义上的区别。 相比小模型，大模型通常参数较多、层数较深，具有更强的表达能力和更高的准确度，但也需要更多的计算资源和时间来训练和推理，适用于数据量较大、计算资源充足的场景，例如云端计算、高性能计算、人工智能等。 2.大模型相关概念区分： **大模型（Large Model,也称基础模型，即 Foundation Model），**是指具有大量参数和复杂结构的机器学习模型，能够处理海量数据、完成各种复杂的任务，如自然语言处理、计算机视觉、语音识别等。 **超大模型：**超大模型是大模型的一个子集，它们的参数量远超过大模型。 **大语言模型（Large Language Model）：**通常是具有大规模参数和计算能力的自然语言处理模型，例如 OpenAI 的 GPT-3 模型。这些模型可以通过大量的数据和参数进行训练，以生成人类类似的文本或回答自然语言的问题。大型语言模型在自然语言处理、文本生成和智能对话等领域有广泛应用。 **GPT（Generative Pre-trained Transformer）：**GPT 和 ChatGPT 都是基于 Transformer 架构的语言模型，但它们在设计和应用上存在区别：GPT 模型旨在生成自然语言文本并处理各种自然语言处理任务，如文本生成、翻译、摘要等。它通常在单向生成的情况下使用，即根据给定的文本生成连贯的输出。 **ChatGPT：**ChatGPT 则专注于对话和交互式对话。它经过特定的训练，以更好地处理多轮对话和上下文理解。ChatGPT 设计用于提供流畅、连贯和有趣的对话体验，以响应用户的输入并生成合适的回复。 3.大模型的发展历程 萌芽期（1950-2005）：以 CNN 为代表的传统神经网络模型阶段 ·1956 年，从计算机专家约翰·麦卡锡提出“人工智能”概念开始，AI 发展由最开始基于小规模专家知识逐步发展为基于机器学习。 ·1980 年，卷积神经网络的雏形 CNN 诞生。 ·1998 年，现代卷积神经网络的基本结构 LeNet-5 诞生，机器学习方法由早期基于浅层机器学习的模型，变为了基于深度学习的模型,为自然语言生成、计算机视觉等领域的深入研究奠定了基础，对后续深度学习框架的迭代及大模型发展具有开创性的意义。 探索沉淀期（2006-2019）：以 Transformer 为代表的全新神经网络模型阶段 ·2013 年，自然语言处理模型Word2Vec 诞生，首次提出将单词转换为向量的“词向量模型”，以便计算机更好地理解和处理文本数据。 ·2014 年，被誉为 21 世纪最强大算法模型之一的 GAN（对抗式生成网络）诞生，标志着深度学习进入了生成模型研究的新阶段。 ·2017 年，Google 颠覆性地提出了基于自注意力机制的神经网络结构——Transformer 架构，奠定了大模型预训练算法架构的基础。 ·2018 年，OpenAI 和 Google 分别发布了 GPT-1 与 BERT 大模型，意味着预训练大模型成为自然语言处理领域的主流。在探索期，以 Transformer 为代表的全新神经网络架构，奠定了大模型的算法架构基础，使大模型技术的性能得到了显著提升。 迅猛发展期（2020-至今）：以 GPT 为代表的预训练大模型阶段 ·2020 年，OpenAI 公司推出了GPT-3，模型参数规模达到了 1750 亿，成为当时最大的语言模型，并且在零样本学习任务上实现了巨大性能提升。随后，更多策略如基于人类反馈的强化学习（RHLF）、代码预训练、指令微调等开始出现, 被用于进一步提高推理能力和任务泛化。 ·2022 年 11 月，搭载了GPT3.5的 ChatGPT横空出世，凭借逼真的自然语言交互与多场景内容生成能力，迅速引爆互联网。 ·2023 年 3 月，最新发布的超大规模多模态预训练大模型——GPT-4，具备了多模态理解与多类型内容生成能力。在迅猛发展期，大数据、大算力和大算法完美结合，大幅提升了大模型的预训练和生成能力以及多模态多场景应用能力。如 ChatGPT 的巨大成功,就是在微软Azure强大的算力以及 wiki 等海量数据支持下，在 Transformer 架构基础上，坚持 GPT 模型及人类反馈的强化学习（RLHF）进行精调的策略下取得的。 4.大模型的特点 · 巨大的规模:大模型包含数十亿个参数，模型大小可以达到数百 GB 甚至更大。巨大的模型规模使大模型具有强大的表达能力和学习能力。 **· 涌现能力：**涌现（英语：emergence）或称创发、突现、呈展、演生，是一种现象，为许多小实体相互作用后产生了大实体，而这个大实体展现了组成它的小实体所不具有的特性。引申到模型层面，涌现能力指的是当模型的训练数据突破一定规模，模型突然涌现出之前小模型所没有的、意料之外的、能够综合分析和解决更深层次问题的复杂能力和特性，展现出类似人类的思维和智能。涌现能力也是大模型最显著的特点之一。 · 更好的性能和泛化能力：大模型通常具有更强大的学习能力和泛化能力，能够在各种任务上表现出色，包括自然语言处理、图像识别、语音识别等。 · 多任务学习:大模型通常会一起学习多种不同的 NLP 任务,如机器翻译、文本摘要、问答系统等。这可以使模型学习到更广泛和泛化的语言理解能力。 · 大数据训练: 大模型需要海量的数据来训练,通常在 TB 以上甚至 PB 级别的数据集。只有大量的数据才能发挥大模型的参数规模优势。 · 强大的计算资源: 训练大模型通常需要数百甚至上千个 GPU,以及大量的时间,通常在几周到几个月。 · 迁移学习和预训练： 大模型可以通过在大规模数据上进行预训练，然后在特定任务上进行微调，从而提高模型在新任务上的性能。 · 自监督学习： 大模型可以通过自监督学习在大规模未标记数据上进行训练，从而减少对标记数据的依赖，提高模型的效能。 ·领域知识融合： 大模型可以从多个领域的数据中学习知识，并在不同领域中进行应用，促进跨领域的创新。 · 自动化和效率：大模型可以自动化许多复杂的任务，提高工作效率，如自动编程、自动翻译、自动摘要等。 5.大模型的分类 按照输入数据类型的不同，大模型主要可以分为以下三大类： ·语言大模型（NLP）：是指在自然语言处理（Natural Language Processing，NLP）领域中的一类大模型，通常用于处理文本数据和理解自然语言。这类大模型的主要特点是它们在大规模语料库上进行了训练，以学习自然语言的各种语法、语义和语境规则。例如：GPT 系列（OpenAI）、Bard（Google）、文心一言（百度）。 ·视觉大模型（CV）：是指在计算机视觉（Computer Vision，CV）领域中使用的大模型，通常用于图像处理和分析。这类模型通过在大规模图像数据上进行训练，可以实现各种视觉任务，如图像分类、目标检测、图像分割、姿态估计、人脸识别等。例如：VIT 系列（Google）、文心UFO、华为盘古 CV、INTERN（商汤）。 **· 多模态大模型：**是指能够处理多种不同类型数据的大模型，例如文本、图像、音频等多模态数据。这类模型结合了 NLP 和 CV 的能力，以实现对多模态信息的综合理解和分析，从而能够更全面地理解和处理复杂的数据。例如：DingoDB 多模向量数据库（九章云极 DataCanvas）、DALL-E(OpenAI)、悟空画画（华为）、midjourney。 按照应用领域的不同，大模型主要可以分为 L0、L1、L2 三个层级： · 通用大模型 L0：是指可以在多个领域和任务上通用的大模型。它们利用大算力、使用海量的开放数据与具有巨量参数的深度学习算法，在大规模无标注数据上进行训练，以寻找特征并发现规律，进而形成可“举一反三”的强大泛化能力，可在不进行微调或少量微调的情况下完成多场景任务，相当于 AI 完成了“通识教育”。 · 行业大模型 L1：是指那些针对特定行业或领域的大模型。它们通常使用行业相关的数据进行预训练或微调，以提高在该领域的性能和准确度，相当于 AI 成为“行业专家”。 · 垂直大模型 L2：是指那些针对特定任务或场景的大模型。它们通常使用任务相关的数据进行预训练或微调，以提高在该任务上的性能和效果。 6.大模型的泛化与微调 **模型的泛化能力：**是指一个模型在面对新的、未见过的数据时，能够正确理解和预测这些数据的能力。在机器学习和人工智能领域，模型的泛化能力是评估模型性能的重要指标之一。 **什么是模型微调：**给定预训练模型（Pre-trained model），基于模型进行微调（Fine Tune）。相对于从头开始训练(Training a model from scatch)，微调可以省去大量计算资源和计算时间，提高计算效率,甚至提高准确率。 模型微调的基本思想是使用少量带标签的数据对预训练模型进行再次训练，以适应特定任务。在这个过程中，模型的参数会根据新的数据分布进行调整。这种方法的好处在于，它利用了预训练模型的强大能力，同时还能够适应新的数据分布。因此，模型微调能够提高模型的泛化能力，减少过拟合现象。 常见的模型微调方法： ·Fine-tuning：这是最常用的微调方法。通过在预训练模型的最后一层添加一个新的分类层，然后根据新的数据集进行微调。 ·Feature augmentation：这种方法通过向数据中添加一些人工特征来增强模型的性能。这些特征可以是手工设计的，也可以是通过自动特征生成技术生成的。 ·Transfer learning：这种方法是使用在一个任务上训练过的模型作为新任务的起点，然后对模型的参数进行微调，以适应新的任务。 大模型是未来人工智能发展的重要方向和核心技术，未来，随着 AI 技术的不断进步和应用场景的不断拓展，大模型将在更多领域展现其巨大的潜力，为人类万花筒般的 AI 未来拓展无限可能性。 AI大模型学习福利 作为一名热心肠的互联网老兵，我决定把宝贵的AI知识分享给大家。 至于能学习到多少就看你的学习毅力和能力了 。我已将重要的AI大模型资料包括AI大模型入门学习思维导图、精品AI大模型学习书籍手册、视频教程、实战学习等录播视频免费分享出来。 因篇幅有限，仅展示部分资料，需要点击下方链接即可前往获取 2024最新版CSDN大礼包：《AGI大模型学习资源包》免费分享 AI大模型时代的学习之旅：从基础到前沿，掌握人工智能的核心技能！ 因篇幅有限，仅展示部分资料，需要点击下方链接即可前往获取 2024最新版CSDN大礼包：《AGI大模型学习资源包》免费分享 这套包含640份报告的合集，涵盖了AI大模型的理论研究、技术实现、行业应用等多个方面。无论您是科研人员、工程师，还是对AI大模型感兴趣的爱好者，这套报告合集都将为您提供宝贵的信息和启示。 因篇幅有限，仅展示部分资料，需要点击下方链接即可前往获取 2024最新版CSDN大礼包：《AGI大模型学习资源包》免费分享 随着人工智能技术的飞速发展，AI大模型已经成为了当今科技领域的一大热点。这些大型预训练模型，如GPT-3、BERT、XLNet等，以其强大的语言理解和生成能力，正在改变我们对人工智能的认识。 那以下这些PDF籍就是非常不错的学习资源。 因篇幅有限，仅展示部分资料，需要点击下方链接即可前往获取 2024最新版CSDN大礼包：《AGI大模型学习资源包》免费分享 因篇幅有限，仅展示部分资料，需要点击下方链接即可前往获取 2024最新版CSDN大礼包：《AGI大模型学习资源包》免费分享 作为普通人，入局大模型时代需要持续学习和实践，不断提高自己的技能和认知水平，同时也需要有责任感和伦理意识，为人工智能的健康发展贡献力量。 立减 ¥ “相关推荐”对你有帮助么？ 热门文章 分类专栏 最新评论 一个小脑袋:博主很专业，文章写的很不错，学习下 Botton ovo:您这个docker源可以发一下吗 aichiroudelibai:王晶被黑得最惨的一次 无敌暴龙战神:解压后，没找到安装包 weixin_46192400:**007** 大家在看 最新文章 目录 目录 分类专栏 目录 请填写红包祝福语或标题 红包个数最小为10个 红包金额最低5元 抵扣说明： 1.余额是钱包充值的虚拟货币，按照1:1的比例进行支付金额的抵扣。2.余额无法直接购买下载，可以购买VIP、付费专栏及课程。\n",
      "==================================================\n",
      "Link: https://blog.csdn.net/Lvbaby_/article/details/131414781\n",
      "OpenAI开发系列（一）：一文搞懂大模型、GPT、ChatGPT等AI概念 全文共5000余字，预计阅读时间约10~20分钟 | 满满干货，建议收藏！ 本文目标：详细解释大型语言模型（LLM）和OpenAI的GPT系列的基本概念。 一、什么是大模型 大型语言模型，也称大语言模型、大模型（Large Language Model，LLM；Large Language Models，LLMs)。 大语言模型是一种深度学习模型，特别是属于自然语言处理（NLP）的领域，一般是指包含数干亿（或更多）参数的语言模型，这些参数是在大量文本数据上训练的，例如模型GPT-3，PaLM，LLaMA等，大语言模型的目的是理解和生成自然语言，通过学习大量的文本数据来预测下一个词或生成与给定文本相关的内容。 参数可以被理解为模型学习任务所需要记住的信息，参数的数量通常与模型的复杂性和学习能力直接相关，更多的参数意味着模型可能具有更强的学习能力。 OpenAI 的 GPT (Generative Pre-trained Transformer) 系列是大语言模型的典型代表，作为目前为止公认最强的 GPT-4 架构，它已经被训练在数十亿的单词上。从实际应用表现来看，大语言模型具备回答各种问题、编写文章、编程、翻译等能力，如果深究其原理，LLM建立在Transformers架构之上，并在很大程度上扩展了模型的大小、预训练数据和总计算量。 可以这么通俗的理解：如果一个模型\"足够大\"，那它就可以称为大模型。 二、什么是GPT GPT，全称\"Generative Pre-training Transformer\"，是一个由OpenAI开发的自然语言处理（NLP）的模型。它的主要目标是理解和生成人类的自然语言。通过对大规模文本数据进行预训练，GPT模型能学习到语言的各种模式，如语法、句法、一词多义等，以及一些基础的世界知识。 总的来说，它通过预训练和生成技术，以及Transformer的自注意力机制，来理解和生成人类的自然语言。 GPT-3 是一个具体的大模型的例子。当提到“大模型”时，通常是指具有大量参数的机器学习模型。 GPT-3是一个特别的大模型，因为它有1750亿个参数。这些参数使得GPT-3在处理语言任务时表现出强大的能力，例如：理解和生成自然语言文本、进行有深度和上下文的对话等。所以，可以说GPT-3是大模型的一个具体应用，显示了大模型的强大能力和可能性。 三、什么是ChatGPT ChatGPT是基于GPT模型构建的基于Web端的“聊天机器人”，对于每一个对话提问，由后端已训练好的GPT3.5或GPT4模型进行预测，并实时返回文字预测的结果，从而实现对话任务。 总的来说，ChatGPT是一个能够生成文本，回答问题和进行自然语言对话的 AI 模型。它可以帮助我们完成各种任务，例如聊天，写作，信息检索和问题回答等。 如需了解更多关于ChatGPT的内容，如发展历程、ChatGPT能做什么等内容，可以看文章：轻松学习ChatGPT：告诉你为什么它能改变你的生活 目前ChatGPT可以使用免费的GPT-3.5和付费的Plus版GPT-4，一张图说明GPT-3.5和GPT-4的区别： ChatGPT是一种特定的GPT应用，GPT是一种大模型，而大模型是一类具有大量参数的深度学习模型。 四、什么是人工智能 人工智能真正进入公众视野的事件是：2017年5月27日阿尔法围棋在与世界排名第一的围棋冠军柯洁进行的人机大战中以3比0的总比分胜出，随后宣布退役。这不仅象征着人工智能在特定领域已经超越了人类的能力，也标志着我们正式步入了人工智能时代。 人工智能，借助机器学习和海量计算能力的强大推动，正逐步提升其复杂性和应用广度。它已经成为推动我们进入新的智能时代的关键力量。全球各地的产业界深谙人工智能技术引领新一轮产业变革的重大意义，都在积极进行转型并提前布局，以适应这个全新的人工智能创新生态。 我们来问一问ChatGPT是怎么描述人工智能的。 Prompt：您好，请帮我解释一下什么是人工智能？ Prompt：您这样解释太专业了，很多人看不懂，能不能更通俗一点、更 生活化一点 ，甚至让儿童都能理解呢？？ 如同蒸汽时代的蒸汽机、电气时代的发电机、信息时代的计算机和互联网，人工智能（AI）正赋能各个产业，推动看人类进入智能时代。 一个经典的人工智能定义是：智能主体可以理解数据及从中学习，并利用知识实现特定目标和任务的能力。 人工智能，被誉为第四次科技革命的核心驱动力，现已步入其2.0时代。我们来回溯一下这个重要的发展轨迹。 在1956年，人工智能这一概念被提出，然而真正的爆发期始于2012年，标志着人工智能1.0时代的启动（2012年-2018年）。此时期的关键里程碑是AlexNet模型的问世，它开启了卷积神经网络（CNN）在图像识别领域的应用。2015年，机器在图像识别的准确率首次超过人类（错误率低于4%），这为计算机视觉技术在各个领域的应用奠定了基础。然而，这个时期的人工智能还面临一些挑战，如模型碎片化和泛化能力不足。 紧接着，人工智能进入了2.0时代（2017年至今）。在这个阶段，Google Brain团队于2017年提出了Transformer架构，这种架构在大模型领域奠定了主流算法基础。自2018年开始，大模型快速崛起，模型参数量呈现指数级增长。比如，2018年谷歌的模型参数首次超过一亿，而到了2022年，模型参数量已经达到了5400亿。这种“预训练+微调”的大模型策略有效地解决了1.0时代人工智能的泛化能力不足问题。伴随着新一代AI技术的不断发展，我们有望迎来全新一轮的技术创新周期。 人工智能产业链主要可划分为基础层、技术层以及应用层这三大部分： 首先，基础层关注基础支持平台的建设，包括传感器、AI芯片、数据服务和计算平台。目前，以约16%的市场份额，浪潮已经成为全球AI基础设施领域的龙头企业，其后分别是戴尔和HPE。 其次，技术层着重于核心技术的研发，主要涵盖算法模型、基础框架以及通用技术。 最后，应用层侧重于行业应用的发展，主要包含行业解决方案服务、硬件产品和软件产品。目前，人工智能在金融、家居、交通、医疗等领域已有重大进展，同时，自动驾驶汽车、无人机、智能机器人、智能语音助手等人工智能产品也在迅速发展。 资料来源：36氪研究院、国信证券经纪研究所 关于市场规模，随着模型参数的不断增加，算力需求也在快速增长。全球AI市场预计在2024年将超过6000亿美元，复合增速为27%。另一方面，中国AI市场预计在2024年将接近八千亿人民币，复合增速达44%，这显然高于全球整体增长速度。 目前，中国人工智能行业的主要竞争者包括浪潮、字节跳动、百度、腾讯、华为、阿里等公司。此外，商汤、旷视、科大讯飞等科技公司也已加入人工智能行业的竞争中。 人工智能已经广泛融入经济发展的各个领域，成为推动科技跨越发展、产业优化升级、生产力整体跃升的重要驱动力量，为新旧动能转换和经济高质量发展提供了有力支撑。具体来说，人工智能的影响主要体现在以下三个方面： 首先，人工智能正在使生产方式变得更加精益化。人工智能技术如自动化、数据驱动决策、实时监控和反馈、智能调度和优化以及连接和协同等，使得生产方式变得更加精益化。这些技术和方法的应用可以提高生产效率、质量控制、资源利用率和生产灵活性，从而推动企业的精益化生产和持续改进。 其次，人工智能正在使生活方式变得更便捷且智能化。近年来，人工智能已经深入到我们的日常生活中，如自动驾驶汽车、智能机器人、语音助手、智能音箱、智能医疗和智能家居等。这些应用不仅提升了我们生活的便捷性，同时也使我们的生活变得更加智能化。 最后，人工智能将会引领新的交往方式的出现。随着人工智能的深层次发展，人与人之间的交往方式正在发生变化。新的交往方式将会出现，由此促进人们交往方式的日新月异。从数字化社交网络到虚拟现实交互，人们的交往方式正在变得越来越多样化，可能会出现新的特殊符号、肢体交往等新式交往模式。 综上所述，人工智能正在全面地改变我们的生产、生活和交往方式，为我们步入高质量的经济社会发展新时代提供了有力的支持和驱动力。 五、什么是AGI与AIGC “AGI” 是 “Artificial General Intelligence” 的缩写，中文通常称为\"人工通用智能\"。AGI是一种理论上的形式的人工智能，指的是能够执行任何人类智能任务的机器。 简单来说，AGI能够理解、学习、适应和应对一切类型的任务，不仅仅是在某个特定的、窄范围的任务上超越人类，比如我们现在见到的大部分AI。有专家预测，GPT-5将会是第一代AGI。 而AIGC，即（AI Generated Content），就是利用AI创造的内容。 六、开源大模型推荐 LLaMa：LLaMa是Meta AI公司开源的一组大规模语言模型，参数范围从7B到65B。它们在多达14,000亿tokens的语料上进行了训练。其中，LLaMA-13B在大部分基准测评上超过了GPT3（175B），6B可在个人GPU上使用，13B时性能相当于GPT3（175B），训练复杂度高于ChatGLM。 ChatGLM:ChatGLM-6B是清华大学知识工程和数据挖掘小组发布的一个开源的、支持中英双语的对话语言模型，基于General Language Model (GLM)架构，具有62亿参数。根据智谱AI公司的GLM-130B修改而来，结合模型量化技术，可以在消费级的显卡上进行本地部署（INT4量化级别下最低只需6GB显存）。 Alpaca：斯坦福大学根据LLAMA7B模型训练得到，性能接近GPT3.5，测试中发现中文支持较差。 MOSS：复旦大学团队开发，是一个支持中英双语和多种插件的开源对话语言模型，moss-moon系列模型具有160亿参数，在FP16精度下可在单张A100/A800或两张3090显卡运行，在INT4/8精度下可在单张3090显卡运行。MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力，同样，经测试对中文支持一般。 miniGPT4：沙特国安大学的华人团队，未公布参数，使用4块A100（80G显存）GPU训练得到，本地部署最低需要12G显存，具备多模态能力。即除了识别文字外，还可识别图像，可以根据图像输出文字。 GPT4ALL：基于LLAMA7B模型微调得到，训练数据采用了OpenAlGPT-3.5-Turbo模型创建的对话语料和其他语料，GPT-3.5-TurboAPI总花费500刀，全部语料均已开源，本地部署需要至少16G显存，缺乏中文训练语料，因此对中文支持较差。 七、为什么需要学习使用开源大模型 首先，目前GPT大模型的使用受到国内外的双重限制，这对于用户的操作空间产生了较大影响。此外，保证数据安全性对于企业来说至关重要，使用GPT大模型可能会存在数据泄露等安全隐患，这无疑增加了使用风险。 其次，在经济层面，使用GPT大模型通常是按量计费的，如果需要大规模使用，就需要支付相对较高的费用。这对于需要控制成本的企业来说，无疑增加了其运营压力。 再次，GPT大模型虽然可以进行微调，但是无法从训练语料层面进行定制化训练。这可能会导致中文对话显得稍显生硬，不够自然，无法满足一些特定需求。 因此，学习并使用开源大模型具有很大的必要性。它们不仅可以帮助我们避免上述问题，还可以根据我们的具体需求进行定制化训练，从而更好地满足我们的需求。 八、当前该如何进行大模型学习 目前大厂提出的大模型都不是开源模型，且大都处于测试阶段。 所以目前学习阶段建议选择OpenAl的GPT模型进行学习和实践。因为其GPT大模型底层原理、参数信息等未公开，所以应围绕API调用、微调方法、数据预处理方法等进行学习和实践，并以应用为主。 九、环境需求 如需继续跟进大模型研发相关技术，您需要具备以下要求： 十、结语 在这篇文章中，我们揭示了人工智能、大模型、GPT、以及ChatGPT的概念及它们潜在关系。希望这篇文章为你解开了这些概念的纷繁复杂，让你对人工智能以及其在我们的工作和生活中的影响有了更深的理解。 最后，感谢您阅读这篇文章！如果您觉得有所收获，别忘了点赞、收藏并关注我，这是我持续创作的动力。您有任何问题或建议，都可以在评论区留言，我会尽力回答并接受您的反馈。如果您希望了解某个特定主题，也欢迎告诉我，我会乐于创作与之相关的文章。谢谢您的支持，期待与您共同成长！ 最后，给大家送上干货！建议大家点赞&收藏，Mark住别丢了。有高质量资料免费送！ 1. 关于魔法，你需要知道的 2. 超全流程！OpenAI账户注册看这里！ 3. ChatGPT Plus 升级指南 立减 ¥ “相关推荐”对你有帮助么？ 热门文章 分类专栏 最新评论 2000nmj:您好，我使用贝叶斯优化时出现了不如随机调参的异常现象，请问是什么原因，应该如何解决 frisk0926:很好的教程 qq_33983085:你好，请问下，SARIMA 中 Jarque-Bera 检验的 P 值不是0.04吗？文中写小于0.01，是不是写错了？ m0_74144991:5.2节里面n阶差分的公式不太对，那个好像是n步差分，正的三角形好像是向上差分，这里应该是向下差分 您愿意向朋友推荐“博客详情页”吗？ 最新文章 目录 目录 分类专栏 目录 请填写红包祝福语或标题 红包个数最小为10个 红包金额最低5元 打赏作者 算法小陈 你的鼓励将是我创作的最大动力 您的余额不足，请更换扫码支付或充值 打赏作者 抵扣说明： 1.余额是钱包充值的虚拟货币，按照1:1的比例进行支付金额的抵扣。2.余额无法直接购买下载，可以购买VIP、付费专栏及课程。\n",
      "==================================================\n",
      "Link: https://blog.csdn.net/youmaob/article/details/137907627\n",
      "什么是大模型？一文读懂大模型的基本概念 大模型是指具有大规模参数和复杂计算结构的机器学习模型。本文从大模型的基本概念出发，对大模型领域容易混淆的相关概念进行区分，并就大模型的发展历程、特点和分类、泛化与微调进行了详细解读，供大家在了解大模型基本知识的过程中起到一定参考作用。 本文目录如下： ·大模型的定义·大模型相关概念区分·大模型的发展历程·大模型的特点·大模型的分类·大模型的泛化与微调 1. 大模型的定义 大模型是指具有大规模参数和复杂计算结构的机器学习模型。这些模型通常由深度神经网络构建而成，拥有数十亿甚至数千亿个参数。大模型的设计目的是为了提高模型的表达能力和预测性能，能够处理更加复杂的任务和数据。大模型在各种领域都有广泛的应用，包括自然语言处理、计算机视觉、语音识别和推荐系统等。大模型通过训练海量数据来学习复杂的模式和特征，具有更强大的泛化能力，可以对未见过的数据做出准确的预测。 ChatGPT对大模型的解释更为通俗易懂，也更体现出类似人类的归纳和思考能力：大模型本质上是一个使用海量数据训练而成的深度神经网络模型，其巨大的数据和参数规模，实现了智能的涌现，展现出类似人类的智能。 那么，大模型和小模型有什么区别？ 小模型通常指参数较少、层数较浅的模型，它们具有轻量级、高效率、易于部署等优点，适用于数据量较小、计算资源有限的场景，例如移动端应用、嵌入式设备、物联网等。 而当模型的训练数据和参数不断扩大，直到达到一定的临界规模后，其表现出了一些未能预测的、更复杂的能力和特性，模型能够从原始训练数据中自动学习并发现新的、更高层次的特征和模式，这种能力被称为“涌现能力”。而具备涌现能力的机器学习模型就被认为是独立意义上的大模型，这也是其和小模型最大意义上的区别。 相比小模型，大模型通常参数较多、层数较深，具有更强的表达能力和更高的准确度，但也需要更多的计算资源和时间来训练和推理，适用于数据量较大、计算资源充足的场景，例如云端计算、高性能计算、人工智能等。 2. 大模型相关概念区分： 大模型（Large Model,也称基础模型，即Foundation Model），是指具有大量参数和复杂结构的机器学习模型，能够处理海量数据、完成各种复杂的任务，如自然语言处理、计算机视觉、语音识别等。 超大模型：超大模型是大模型的一个子集，它们的参数量远超过大模型。 大语言模型（Large Language Model）：通常是具有大规模参数和计算能力的自然语言处理模型，例如 OpenAI 的 GPT-3 模型。这些模型可以通过大量的数据和参数进行训练，以生成人类类似的文本或回答自然语言的问题。大型语言模型在自然语言处理、文本生成和智能对话等领域有广泛应用。 GPT（Generative Pre-trained Transformer）：GPT 和ChatGPT都是基于Transformer架构的语言模型，但它们在设计和应用上存在区别：GPT模型旨在生成自然语言文本并处理各种自然语言处理任务，如文本生成、翻译、摘要等。它通常在单向生成的情况下使用，即根据给定的文本生成连贯的输出。 ChatGPT：ChatGPT则专注于对话和交互式对话。它经过特定的训练，以更好地处理多轮对话和上下文理解。ChatGPT设计用于提供流畅、连贯和有趣的对话体验，以响应用户的输入并生成合适的回复。 3. 大模型的发展历程 萌芽期（1950-2005）：以CNN为代表的传统神经网络模型阶段 · 1956年，从计算机专家约翰·麦卡锡提出“人工智能”概念开始，AI发展由最开始基于小规模专家知识逐步发展为基于机器学习。 · 1980年，卷积神经网络的雏形CNN诞生。 · 1998年，现代卷积神经网络的基本结构LeNet-5诞生，机器学习方法由早期基于浅层机器学习的模型，变为了基于深度学习的模型,为自然语言生成、计算机视觉等领域的深入研究奠定了基础，对后续深度学习框架的迭代及大模型发展具有开创性的意义。 探索沉淀期（2006-2019）：以Transformer为代表的全新神经网络模型阶段 · 2013年，自然语言处理模型 Word2Vec诞生，首次提出将单词转换为向量的“词向量模型”，以便计算机更好地理解和处理文本数据。 · 2014年，被誉为21世纪最强大算法模型之一的GAN（对抗式生成网络）诞生，标志着深度学习进入了生成模型研究的新阶段。 · 2017年，Google颠覆性地提出了基于自注意力机制的神经网络结构——Transformer架构，奠定了大模型预训练算法架构的基础。 · 2018年，OpenAI和Google分别发布了GPT-1与BERT大模型，意味着预训练大模型成为自然语言处理领域的主流。在探索期，以Transformer为代表的全新神经网络架构，奠定了大模型的算法架构基础，使大模型技术的性能得到了显著提升。 迅猛发展期（2020-至今）：以GPT为代表的预训练大模型阶段 · 2020年，OpenAI公司推出了GPT-3，模型参数规模达到了1750亿，成为当时最大的语言模型，并且在零样本学习任务上实现了巨大性能提升。随后，更多策略如基于人类反馈的强化学习（RHLF）、代码预训练、指令微调等开始出现, 被用于进一步提高推理能力和任务泛化。 · 2022年11月，搭载了GPT3.5的ChatGPT横空出世，凭借逼真的自然语言交互与多场景内容生成能力，迅速引爆互联网。 · 2023年3月，最新发布的超大规模多模态预训练大模型——GPT-4，具备了多模态理解与多类型内容生成能力。在迅猛发展期，大数据、大算力和大算法完美结合，大幅提升了大模型的预训练和生成能力以及多模态多场景应用能力。如ChatGPT的巨大成功,就是在微软Azure强大的算力以及wiki等海量数据支持下，在Transformer架构基础上，坚持GPT模型及人类反馈的强化学习（RLHF）进行精调的策略下取得的。 4. 大模型的特点 ·巨大的规模: 大模型包含数十亿个参数，模型大小可以达到数百GB甚至更大。巨大的模型规模使大模型具有强大的表达能力和学习能力。 · 涌现能力：涌现（英语：emergence）或称创发、突现、呈展、演生，是一种现象，为许多小实体相互作用后产生了大实体，而这个大实体展现了组成它的小实体所不具有的特性。引申到模型层面，涌现能力指的是当模型的训练数据突破一定规模，模型突然涌现出之前小模型所没有的、意料之外的、能够综合分析和解决更深层次问题的复杂能力和特性，展现出类似人类的思维和智能。涌现能力也是大模型最显著的特点之一。 · 更好的性能和泛化能力： 大模型通常具有更强大的学习能力和泛化能力，能够在各种任务上表现出色，包括自然语言处理、图像识别、语音识别等。 · 多任务学习: 大模型通常会一起学习多种不同的NLP任务,如机器翻译、文本摘要、问答系统等。这可以使模型学习到更广泛和泛化的语言理解能力。 · 大数据训练: 大模型需要海量的数据来训练,通常在TB以上甚至PB级别的数据集。只有大量的数据才能发挥大模型的参数规模优势。 · 强大的计算资源: 训练大模型通常需要数百甚至上千个GPU,以及大量的时间,通常在几周到几个月。 · 迁移学习和预训练： 大模型可以通过在大规模数据上进行预训练，然后在特定任务上进行微调，从而提高模型在新任务上的性能。 · 自监督学习： 大模型可以通过自监督学习在大规模未标记数据上进行训练，从而减少对标记数据的依赖，提高模型的效能。 · 领域知识融合： 大模型可以从多个领域的数据中学习知识，并在不同领域中进行应用，促进跨领域的创新。 · 自动化和效率：大模型可以自动化许多复杂的任务，提高工作效率，如自动编程、自动翻译、自动摘要等。 5. 大模型的分类 按照输入数据类型的不同，大模型主要可以分为以下三大类： · 语言大模型（NLP）：是指在自然语言处理（Natural Language Processing，NLP）领域中的一类大模型，通常用于处理文本数据和理解自然语言。这类大模型的主要特点是它们在大规模语料库上进行了训练，以学习自然语言的各种语法、语义和语境规则。例如：GPT系列（OpenAI）、Bard（Google）、文心一言（百度）。 · 视觉大模型（CV）：是指在计算机视觉（Computer Vision，CV）领域中使用的大模型，通常用于图像处理和分析。这类模型通过在大规模图像数据上进行训练，可以实现各种视觉任务，如图像分类、目标检测、图像分割、姿态估计、人脸识别等。例如：VIT系列（Google）、文心UFO、华为盘古CV、INTERN（商汤）。 · 多模态大模型：是指能够处理多种不同类型数据的大模型，例如文本、图像、音频等多模态数据。这类模型结合了NLP和CV的能力，以实现对多模态信息的综合理解和分析，从而能够更全面地理解和处理复杂的数据。例如：DingoDB多模向量数据库（九章云极DataCanvas）、DALL-E(OpenAI)、悟空画画（华为）、midjourney。 按照应用领域的不同，大模型主要可以分为L0、L1、L2三个层级： · 通用大模型L0：是指可以在多个领域和任务上通用的大模型。它们利用大算力、使用海量的开放数据与具有巨量参数的深度学习算法，在大规模无标注数据上进行训练，以寻找特征并发现规律，进而形成可“举一反三”的强大泛化能力，可在不进行微调或少量微调的情况下完成多场景任务，相当于AI完成了“通识教育”。 · 行业大模型L1：是指那些针对特定行业或领域的大模型。它们通常使用行业相关的数据进行预训练或微调，以提高在该领域的性能和准确度，相当于AI成为“行业专家”。 · 垂直大模型L2：是指那些针对特定任务或场景的大模型。它们通常使用任务相关的数据进行预训练或微调，以提高在该任务上的性能和效果。 6. 大模型的泛化与微调 模型的泛化能力：是指一个模型在面对新的、未见过的数据时，能够正确理解和预测这些数据的能力。在机器学习和人工智能领域，模型的泛化能力是评估模型性能的重要指标之一。 什么是模型微调：给定预训练模型（Pre-trained model），基于模型进行微调（Fine Tune）。相对于从头开始训练(Training a model from scatch)，微调可以省去大量计算资源和计算时间，提高计算效率,甚至提高准确率。 模型微调的基本思想是使用少量带标签的数据对预训练模型进行再次训练，以适应特定任务。在这个过程中，模型的参数会根据新的数据分布进行调整。这种方法的好处在于，它利用了预训练模型的强大能力，同时还能够适应新的数据分布。因此，模型微调能够提高模型的泛化能力，减少过拟合现象。 常见的模型微调方法： · Fine-tuning：这是最常用的微调方法。通过在预训练模型的最后一层添加一个新的分类层，然后根据新的数据集进行微调。 · Feature augmentation：这种方法通过向数据中添加一些人工特征来增强模型的性能。这些特征可以是手工设计的，也可以是通过自动特征生成技术生成的。 · Transfer learning：这种方法是使用在一个任务上训练过的模型作为新任务的起点，然后对模型的参数进行微调，以适应新的任务。 大模型是未来人工智能发展的重要方向和核心技术，未来，随着AI技术的不断进步和应用场景的不断拓展，大模型将在更多领域展现其巨大的潜力，为人类万花筒般的AI未来拓展无限可能性。 大模型岗位需求 大模型时代，企业对人才的需求变了，AIGC相关岗位人才难求，薪资持续走高，AI运营薪资平均值约18457元，AI工程师薪资平均值约37336元，大模型算法薪资平均值约39607元。 掌握大模型技术你还能拥有更多可能性： • 成为一名全栈大模型工程师，包括Prompt，LangChain，LoRA等技术开发、运营、产品等方向全栈工程； • 能够拥有模型二次训练和微调能力，带领大家完成智能对话、文生图等热门应用； • 薪资上浮10%-20%，覆盖更多高薪岗位，这是一个高需求、高待遇的热门方向和领域； • 更优质的项目可以为未来创新创业提供基石。 可能大家都想学习AI大模型技术，也想通过这项技能真正达到升职加薪，就业或是副业的目的，但是不知道该如何开始学习，因为网上的资料太多太杂乱了，如果不能系统的学习就相当于是白学。为了让大家少走弯路，少碰壁，这里我直接把全套AI技术和大模型入门资料、操作变现玩法都打包整理好，希望能够真正帮助到大家。 -END- 大模型学习路线图，整体分为7个大的阶段：（全套教程文末领取哈）第一阶段：从大模型系统设计入手，讲解大模型的主要方法； 第二阶段：在通过大模型提示词工程从Prompts角度入手更好发挥模型的作用； 第三阶段：大模型平台应用开发借助阿里云PAI平台构建电商领域虚拟试衣系统； 第四阶段：大模型知识库应用开发以LangChain框架为例，构建物流行业咨询智能问答系统； 第五阶段：大模型微调开发借助以大健康、新零售、新媒体领域构建适合当前领域大模型； 第六阶段：以SD多模态大模型为主，搭建了文生图小程序案例； 第七阶段：以大模型平台应用与开发为主，通过星火大模型，文心大模型等成熟大模型构建大模型行业应用。 👉大模型实战案例👈 光学理论是没用的，要学会跟着一起做，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。 👉大模型视频和PDF合集👈 观看零基础学习书籍和视频，看书籍和视频学习是最快捷也是最有效果的方式，跟着视频中老师的思路，从基础到深入，还是很容易入门的。 👉学会后的收获：👈 • 基于大模型全栈工程实现（前端、后端、产品经理、设计、数据分析等），通过这门课可获得不同能力； • 能够利用大模型解决相关实际项目需求：大数据时代，越来越多的企业和机构需要处理海量数据，利用大模型技术可以更好地处理这些数据，提高数据分析和决策的准确性。因此，掌握大模型应用开发技能，可以让程序员更好地应对实际项目需求； • 基于大模型和企业数据AI应用开发，实现大模型理论、掌握GPU算力、硬件、LangChain开发框架和项目实战技能，学会Fine-tuning垂直训练大模型（数据准备、数据蒸馏、大模型部署）一站式掌握； • 能够完成时下热门大模型垂直领域模型训练能力，提高程序员的编码能力：大模型应用开发需要掌握机器学习算法、深度学习框架等技术，这些技术的掌握可以提高程序员的编码能力和分析能力，让程序员更加熟练地编写高质量的代码。 👉获取方式： 😝有需要的小伙伴，可以保存图片到wx扫描二v码免费领取【保证100%免费】🆓 保证100%免费 立减 ¥ “相关推荐”对你有帮助么？ 热门文章 分类专栏 最新评论 在网络中摸爬滚打的小强:要不是我有其中的证书，我差点信了去考了卷啊卷 m0_71104086:你好，请问火车票的代码在哪找 我是小飞熊:如果文件太多，系统运行会变慢，请问如何解决性能问题？ 我是小飞熊:为啥不上传代码，只上传图片？ 网瘾少女网工:这个讲得好详细，前期新手准备的工具都有介绍和链接，爱了爱了 大家在看 最新文章 目录 目录 分类专栏 目录 请填写红包祝福语或标题 红包个数最小为10个 红包金额最低5元 抵扣说明： 1.余额是钱包充值的虚拟货币，按照1:1的比例进行支付金额的抵扣。2.余额无法直接购买下载，可以购买VIP、付费专栏及课程。\n",
      "==================================================\n",
      "Link: https://cloud.tencent.com/developer/article/2362076\n",
      "原创 | 大模型扫盲系列——初识大模型 原创 | 大模型扫盲系列——初识大模型 作者：金一鸣本文约6700字，建议阅读10+分钟本文将从大模型的原理、训练过程、prompt和相关应用介绍等方面进行分析，帮助读者初步了解大模型。 近年来，随着计算机技术和大数据的快速发展，深度学习在各个领域取得了显著的成果。为了提高模型的性能，研究者们不断尝试增加模型的参数数量，从而诞生了大模型这一概念。本文将从大模型的原理、训练过程、prompt和相关应用介绍等方面进行分析，帮助读者初步了解大模型。 大模型的定义 大模型是指具有数千万甚至数亿参数的深度学习模型。近年来，随着计算机技术和大数据的快速发展，深度学习在各个领域取得了显著的成果，如自然语言处理，图片生成，工业数字化等。为了提高模型的性能，研究者们不断尝试增加模型的参数数量，从而诞生了大模型这一概念。本文讨论的大模型将以平时指向比较多的大语言模型为例来进行相关介绍。 大模型的基本原理与特点 大模型的原理是基于深度学习，它利用大量的数据和计算资源来训练具有大量参数的神经网络模型。通过不断地调整模型参数，使得模型能够在各种任务中取得最佳表现。通常说的大模型的“大”的特点体现在：参数数量庞大、训练数据量大、计算资源需求高等。很多先进的模型由于拥有很“大”的特点，使得模型参数越来越多，泛化性能越来越好，在各种专门的领域输出结果也越来越准确。现在市面上比较流行的任务有AI生成语言（ChatGPT类产品）、AI生成图片（Midjourney类产品）等，都是围绕生成这个概念来展开应用。“生成”简单来说就是根据给定内容，预测和输出接下来对应内容的能力。比如最直观的例子就是成语接龙，可以把大语言模型想象成成语接龙功能的智能版本，也就是根据最后一个字输出接下来一段文章或者一个句子。 一个基本架构，三种形式： 当前流行的大模型的网络架构其实并没有很多新的技术，还是一直沿用当前NLP领域最热门最有效的架构——Transformer结构。相比于传统的循环神经网络（RNN）和长短时记忆网络（LSTM），Transformer具有独特的注意力机制（Attention），这相当于给模型加强理解力，对更重要的词能给予更多关注，同时该机制具有更好的并行性和扩展性，能够处理更长的序列，立马成为NLP领域具有奠基性能力的模型，在各类文本相关的序列任务中取得不错的效果。 根据这种网络架构的变形，主流的框架可以分为Encoder-Decoder, Encoder-Only和Decoder-Only，其中： 1）Encoder-Only，仅包含编码器部分，主要适用于不需要生成序列的任务，只需要对输入进行编码和处理的单向任务场景，如文本分类、情感分析等，这类代表是BERT相关的模型，例如BERT，RoBERT，ALBERT等 2）Encoder-Decoder，既包含编码器也包含解码器，通常用于序列到序列（Seq2Seq）任务，如机器翻译、对话生成等，这类代表是以Google训出来T5为代表相关大模型。 3）Decoder-Only，仅包含解码器部分，通常用于序列生成任务，如文本生成、机器翻译等。这类结构的模型适用于需要生成序列的任务，可以从输入的编码中生成相应的序列。同时还有一个重要特点是可以进行无监督预训练。在预训练阶段，模型通过大量的无标注数据学习语言的统计模式和语义信息。这种方法可以使得模型具备广泛的语言知识和理解能力。在预训练之后，模型可以进行有监督微调，用于特定的下游任务（如机器翻译、文本生成等）。这类结构的代表也就是我们平时非常熟悉的GPT模型的结构，所有该家族的网络结构都是基于Decoder-Only的形式来逐步演化。 可以看到，很多NLP任务可能可以通过多种网络结果来解决，这也主要是因为NLP领域的任务和数据的多样性和复杂性，以及现代深度学习模型的灵活性和泛化能力，具体哪种结构有效，一般需要根据具体场景和数据，通过实验效果进行选择。 训练三步骤 初步认识了大模型长什么样了，接下来一起来看看如何训练出一个大模型。 训练方式，这里主要参考OpenAI发表的关于InstructGPT的相关训练步骤，主流的大模型训练基本形式大多也是类似的： 1、预训练（Pretraining） 预训练是大模型训练的第一步，目的是让模型学习语言的统计模式和语义信息。主流的预训练阶段步骤基本都是近似的，其中最重要的就是数据，需要收集大量的无标注数据，例如互联网上的文本、新闻、博客、论坛等等。这些数据可以是多种语言的，并且需要经过一定的清洗和处理，以去除噪音，无关信息以及个人隐私相关的，最后会以tokenizer粒度输入到上文提到的语言模型中。这些数据经过清洗和处理后，用于训练和优化语言模型。预训练过程中，模型会学习词汇、句法和语义的规律，以及上下文之间的关系。OpenAI的ChatGPT4能有如此惊人的效果，主要的一个原因就是他们训练数据源比较优质。 2、 指令微调阶段（Instruction Tuning Stage） 在完成预训练后，就可以通过指令微调去挖掘和增强语言模型本身具备的能力，这步也是很多企业以及科研研究人员利用大模型的重要步骤。 Instruction tuning（指令微调）是大模型训练的一个阶段，它是一种有监督微调的特殊形式，旨在让模型理解和遵循人类指令。在指令微调阶段，首先需要准备一系列的NLP任务，并将每个任务转化为指令形式，其中指令包括人类对模型应该执行的任务描述和期望的输出结果。然后，使用这些指令对已经预训练好的大语言模型进行监督学习，使得模型通过学习和适应指令来提高其在特定任务上的表现。 为了让模型训练更加高效和简单，这个阶段还有一种高效的fine-tuning技术，这为普通的从业者打开了通向使用大模型的捷径。 Parameter-Efficient Fine-Tuning (PEFT)旨在通过最小化微调参数的数量和计算复杂度，达到高效的迁移学习的目的，提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。在训练过程中，预训练模型的参数保持不变，只需微调少量的额外参数，就可以达到与全量微调相当的性能。 目前，很多研究对PEFT方法进行了探索，例如Adapter Tuning和Prefix Tuning等。其中，Adapter Tuning方法在面对特定的下游任务时，将预训练模型中的某些层固定，只微调接近下游任务的几层参数。而Prefix Tuning方法则是在预训练模型的基础上，添加一些额外的参数，这些参数在训练过程中会根据特定的任务进行更新和调整。 工业界现在常用的Adapter Tuning的技术是Low-Rank Adaptation（LoRA） 。它通过最小化微调参数的数量和计算复杂度，实现高效的迁移学习，以提高预训练模型在新任务上的性能。LoRA 的核心思想是将预训练模型的权重矩阵分解为两个低秩矩阵的乘积。通过这种分解，可以显著减少微调参数的数量，并降低计算复杂度。该方式和机器学习中经典的降维的思想很类似，类似地，LoRA 使用了矩阵分解技术中的奇异值分解 (Singular Value Decomposition, SVD) 或低秩近似 (Low-Rank Approximation) 方法，将原始权重矩阵分解为两个低秩矩阵的乘积。 在微调过程中，LoRA 只更新这两个低秩矩阵的参数，而保持其他预训练参数固定不变。这样可以显著减少微调所需的计算资源和时间，并且在很多任务上取得了与全量微调相当的性能。 LoRA技术的引入使得在大规模预训练模型上进行微调更加高效和可行，为实际应用提供了更多可能性。 3、对齐微调（Alignment Tuning） 主要目标在于将语言模型与人类的偏好、价值观进行对齐，其中最重要的技术就是使用RLHF（reinforcement learning from human feedback）来进行对齐微调。 Step 1.预训练模型的有监督微调 先收集一个提示词集合，并要求标注人员写出高质量的回复，然后使用该数据集以监督的方式微调预训练的基础模型。 Step 2.训练奖励模型 这个过程涉及到与人类评估者进行对话，并根据他们的反馈来进行调整和优化。评估者会根据个人偏好对模型生成的回复进行排序，从而指导模型生成更符合人类期望的回复。这种基于人类反馈的训练方式可以帮助模型捕捉到更多人类语言的特点和习惯，从而提升模型的生成能力。 Step 3.利用强化学习模型微调 主要使用了强化学习的邻近策略优化（PPO，proximal policy optimization ）算法，对于每个时间步，PPO算法会计算当前产生和初始化的KL散度，根据这个分布来计算一个状态或动作的预期回报，然后使用这个回报来更新策略，达到对SFT模型进一步优化。 但是这种算法存在一些比较明显的缺点，比如PPO是on-policy算法，每一次更新都需要收集新的样本，这就会导致算法的效率低下，并且更新是在每次训练时进行的，因此策略更新比较频繁，这就会导致算法的稳定性较差。 所以当前有很多新的技术出来替代RLHF技术： 直接偏好优化（DPO）是一种对传统RLHF替代的技术，作者在论文中提出拟合一个反映人类偏好的奖励模型，将奖励函数和最优策略之间的映射联系起来，从而把约束奖励最大化问题转化为一个单阶段的策略训练问题。然后通过强化学习来微调大型无监督语言模型，以最大化这个预估的奖励。这个算法具有简单有效和计算轻量级的特点，不需要拟合奖励模型，只需要进行单阶段训练，也不需要大量的超参数调节，所以在响应质量方面也通常优于传统的RLHF。另外还有RLAIF从采样方式，生成训练奖励模型的评分的角度来替代原有的PPO的RLHF进行训练。 DPO方法 对齐微调是一个关键的阶段，这一阶段使用强化学习从人类反馈中进行微调，以进一步优化模型的生成能力。它通过与人类评估者和用户的互动，不断优化模型的生成能力，以更好地满足人类期望和需求。 Prompt 作为大模型的一个技术分支，很多人接触大模型的第一步就是写prompt，而这的确也是大模型发展的其中一个重要方向技术，也是很多实际运用问题解决的关键步骤。 Prompt技术的基本思想是，通过给模型提供一个或多个提示词或短语，来指导模型生成符合要求的输出。本质上是通过恰当的初始化参数（也就是适当的输入语言描述），来激发语言模型本身的潜力。例如，在文本分类任务中，我们可以给模型提供一个类别标签的列表，并要求它生成与这些类别相关的文本；在机器翻译任务中，我们可以给模型提供目标语言的一段文本，并要求它翻译这段文本。 Prompt根据常用的使用场景可以概括为以下四种： Zero-Shot Prompt:在零样本场景下使用，模型根据提示或指令进行任务处理，不需要针对每个新任务或领域都进行专门的训练，这类一般作为训练通用大模型的最常见的评估手段。 Few-Shot Prompt:在少样本场景下使用，模型从少量示例中学习特定任务，利用迁移学习的方法来提高泛化性能，该类prompt也是很多实际应用案例都采取来进行大模型微调训练的方式。 Chain-of-thought prompt：这类prompt常见于推理复杂任务，它通过引导模型逐步解决问题，以一系列连贯的步骤展示推理的思路和逻辑关系。通过这种逐步推理的方式，模型可以逐渐获得更多信息，并在整个推理过程中累积正确的推断。 Multimodal prompt：这类prompt包含的信息就更丰富，主要是将不同模态的信息（如文本、图像、音频等）融合到一起，形成一种多模态的提示，以帮助模型更好地理解和处理输入数据。比如在问答系统中，可以将问题和相关图像作为多模态输入，以帮助模型更好地理解问题的含义和上下文背景，并生成更加准确和全面的答案。 在具体实践中，根据场景设计合适的prompt进行优化，评估也是大模型工程中重要的一步，对大模型准确率和可靠性提升是必不可少的，这步也是将模型潜在强大能力兑现的关键一环。 大模型应用 当前大模型已经在很多领域开始产品化落地，除了ChatGPT这类大家熟知的产品，主要还有以下一些主流的应用： 1）办公Copilot类产品：微软首先尝试使用大模型能力来接入旗下的Office系列软件，在Word中可以对文档进行总结并提出修改编辑的建议，也可以对所给的文章进行总结；此前一直头疼Excel各种复杂操作的用户现在也降低了使用门槛，可以直接通过描述就处理数据；PowerPoint中通过对提出要求识别就能自动生成一份展示内容；在Outlook中直接使用自然语言来生成邮件内容等功能，实现真正的AI秘书。 2）Github Copilot类产品：直接通过对话方式进行各种功能代码的生成，包括帮忙写测试用例，解释代码片段和debug程序问题，这个功能对解放程序员生产力取得了革命性的进步，能让开发人员更多的关注到业务理解，系统设计，架构设计等更高级需求的事情上。 2）教育知识类产品：得益于大模型强大的理解以及知识储备，很多公司也嵌入其知识类产品进行应用，比如chatPDF就可以帮助经常看论文的科研人员快速地通过问答的方式进行文章的信息提取，理解以及总结重要内容，大大提升了阅读新论文的效率；对于学习语言的人来说，一款叫Call Annie的软件基本能取代口语老师的角色，并且可以无限时间，随时随地进行口语对话练习。 4）搜索引擎和推荐系统：大模型可以应用于企业的搜索引擎和推荐系统，通过深度学习算法，对用户的搜索意图进行准确理解，提供更精准的搜索结果和个性化的推荐内容。这有助于提升用户体验，增加用户黏性，提高企业的转化率和销售额。 5）公司业务定制化大模型：大模型具有通用性能力，但是在很多零样本的场景的表现依然比不上那个领域正在使用的产品，例如在某些垂直领域，包括工业领域，医药领域，管理领域等场景下进行专业问题，研究型问题的使用依然需要特定场景的数据进行微调，这种定制化的服务也能给企业带来巨大的效率提升和节省成本的收益，属于比较有前景的业务。 6）计算相关上下游相关产业：很多公司正在积极探索基于GPU、FPGA和ASIC等硬件加速制造技术，以支持大模型的训练和推理速度。此外，云计算技术的发展也为大模型的训练提供了更多的计算资源支持，未来科技公司将积极探索基于云计算的分布式训练和推理技术。 除这些外还包括算法优化、隐私和数据安全以及模型可解释性等方面的研究和应用，每天还有很多大模型的应用正在不断涌现，大模型在未来仍然有很大的发展潜力，国内的优秀大模型代表例如百度文心大模型也正在搭建全系统产业化的大模型全景。 大模型挑战 大模型也存在一些现实挑战： 1.数据安全隐患：一方面大模型训练需要大量的数据支持，但很多数据涉及到机密以及个人隐私问题，如客户信息、交易数据等。需要保证在训练大模型的同时保障数据安全，防止数据泄露和滥用。OpenAI在发布ChatGPT模型的时候用了数月来保证数据安全以及符合人类正常价值观标准。 2.成本高昂：大模型的训练和部署需要大量的计算资源和人力资源，成本非常高昂。对于一些中小型企业而言，难以承担这些成本，也难以获得足够的技术支持和资源。 3.无法保障内容可信：大模型会编造词句，无法保障内容真实可信、有据可查。当前使用者只能根据自己需求去验证生成的内容是否真实可信，很难具有权威说服力。 4.无法实现成本可控：直接训练和部署千亿级参数大模型成本过高，企业级应用应使用百亿级基础模型，根据不同需求训练不同的垂直模型，企业则只需要负担垂直训练成本。但是，如何实现高效的垂直训练，如何控制成本，仍是大模型面临的问题之一。 以上挑战依然有很大空间值得改进，需要进一步研究和探索新的技术和方法。比如可以采用数据加密、隐私保护等技术来保障数据安全；可以通过改进模型架构、优化训练算法、利用分布式计算等方式来提高大模型的效率和性能；此外，还可以通过开源和共享模型资源来降低成本、促进大模型的普及和应用等方式。 总结 最后，大模型的发展是当前人工智能时代科技进步的必然趋势，甚至可以媲美工业革命般的历史意义。近期，有MIT的研究者发现语言模型竟然能理解这个世界的时间和空间，这项研究也进一步说明大模型还有很多隐藏的能力等着我们去发掘。长期看，训练出通用人工智能技术（AGI）应该只是时间问题。作为相关从业人员，可以开发更高效，更稳定的训练算法，不断探索大模型的上限，作为普通人，我们更需要拥抱这个技术，至少在日常工作和生活中也能享受到其带来的巨大便利。 参考： Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, Ł. and Polosukhin, I. (2017).Attention Is All You Need. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J. and Lowe, R. (2022). Training language models to follow instructions with human feedback.arXiv:2203.02155 [cs]. [online] Available at: https://arxiv.org/abs/2203.02155. Houlsby, N., Giurgiu, A., Jastrzȩbski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M. and Gelly, S. (n.d.).Parameter-Efficient Transfer Learning for NLP. [online] Available at: http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models.arXiv:2106.09685 [cs]. [online] Available at: https://arxiv.org/abs/2106.09685. Openai, C., Deepmind, J., Brown, T., Deepmind, M., Deepmind, S. and Openai, D. (n.d.).Deep Reinforcement Learning from Human Preferences. [online] Available at: https://arxiv.org/pdf/1706.03741.pdf. ‌Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C., Finn, C., & Cz Biohub. (n.d.). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. https://arxiv.org/pdf/2305.18290.pdf Xiang, L., Li and Liang, P. (n.d.).Prefix-Tuning: Optimizing Continuous Prompts for Generation. [online] Available at: https://arxiv.org/pdf/2101.00190.pdf. Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z. and Liu, P. (2023). A Survey of Large Language Models.arXiv:2303.18223 [cs]. [online] Available at: https://arxiv.org/abs/2303.18223. ‌Gulcehre, C., Paine, T.L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C., Macherey, W., Doucet, A., Firat, O. and de Freitas, N. (2023).Reinforced Self-Training (ReST) for Language Modeling. [online] arXiv.org. doi:https://doi.org/10.48550/arXiv.2308.08998. ‌Lee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T., Bishop, C., Carbune, V. and Rastogi, A. (2023).RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. [online] arXiv.org. doi:https://doi.org/10.48550/arXiv.2309.00267. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q. and Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models.arXiv:2201.11903 [cs]. [online] Available at: https://arxiv.org/abs/2201.11903. ‌ Khattak, M., Rasheed, H., Maaz, M., Khan, S., Fahad, S. and Khan (n.d.).MaPLe: Multi-modal Prompt Learning. [online] Available at: https://arxiv.org/pdf/2210.03117.pdf [Accessed 21 Oct. 2023]. ‌ Gurnee, W. and Tegmark, M. (n.d.).LANGUAGE MODELS REPRESENT SPACE AND TIME. [online] Available at: https://arxiv.org/pdf/2310.02207.pdf [Accessed 15 Oct. 2023]. ‌ 编辑：黄继彦 数据派研究部介绍 数据派研究部成立于2017年初，以兴趣为核心划分多个组别，各组既遵循研究部整体的知识分享和实践项目规划，又各具特色： 算法模型组：积极组队参加kaggle等比赛，原创手把手教系列文章； 调研分析组：通过专访等方式调研大数据的应用，探索数据产品之美； 系统平台组：追踪大数据&人工智能系统平台技术前沿，对话专家； 自然语言处理组：重于实践，积极参加比赛及策划各类文本分析项目； 制造业大数据组：秉工业强国之梦，产学研政结合，挖掘数据价值； 数据可视化组：将信息与艺术融合，探索数据之美，学用可视化讲故事； 网络爬虫组：爬取网络信息，配合其他各组开发创意项目。 点击文末“阅读原文”，报名数据派研究部志愿者，总有一组适合你~ 转载须知 如需转载，请在开篇显著位置注明作者和出处（转自：数据派THUID：DatapiTHU），并在文章结尾放置数据派醒目二维码。有原创标识文章，请发送【文章名称-待授权公众号名称及ID】至联系邮箱，申请白名单授权并按要求编辑。 未经许可的转载以及改编者，我们将依法追究其法律责任。 点击“阅读原文”加入组织~ 本文分享自数据派THU微信公众号，前往查看 如有侵权，请联系cloudcommunity@tencent.com删除。 本文参与腾讯云自媒体同步曝光计划，欢迎热爱写作的你一起参与！ 社区 活动 资源 关于 腾讯云开发者 扫码关注腾讯云开发者 领取腾讯云代金券 热门产品 热门推荐 更多推荐 Copyright © 2013 -2024Tencent Cloud. All Rights Reserved. 腾讯云 版权所有 深圳市腾讯计算机系统有限公司 ICP备案/许可证号：粤B2-20090059深公网安备号 44030502008569 腾讯云计算（北京）有限责任公司 京ICP证150476号 |京ICP备11018762号|京公网安备号11010802020287 Copyright © 2013 -2024Tencent Cloud. All Rights Reserved. 腾讯云 版权所有\n",
      "==================================================\n",
      "\n",
      "搜索链接：['https://blog.csdn.net/leah126/article/details/139140426', 'https://blog.csdn.net/Lvbaby_/article/details/131414781', 'https://www.aigc.cn/large-models', 'https://blog.csdn.net/youmaob/article/details/137907627', 'https://cloud.tencent.com/developer/article/2362076']\n"
     ]
    }
   ],
   "source": [
    "print(get_bing_searched_results(keyword='大模型', max_results=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
