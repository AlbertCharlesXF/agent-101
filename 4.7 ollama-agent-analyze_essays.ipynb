{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意事项\n",
    "- 需要提前安装好ollama: https://ollama.com/\n",
    "- 在cmd运行：ollama run gemma2，即可安装开源大模型gemma2\n",
    "- 支持各类强大的开源大模型：\n",
    "    - internlm2\n",
    "    - codegeex4\n",
    "    - glm4\n",
    "    - llama3\n",
    "    - qwen2\n",
    "    - deepseek-coder-v2\n",
    "    - phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 让开源大模型安全地润色我们的论文\n",
    "\n",
    "我们今天来讨论如何让安全地让大模型帮我们改论文~处理机密数据\n",
    "\n",
    "在处理敏感信息的时候比如：\n",
    "- 将要发布的论文\n",
    "- 企业核心数据\n",
    "如果使用闭源大模型的API，可能会有数据泄露的风险。（多数大模型公司都会保护用户隐私，但是也不可避免存在风险）\n",
    "如果担心这类风险，我们可以用开源大模型，直接部署到本地的电脑或者服务器中，离线运行，直接用这些模型自身生成回答，避免传输到网络中。\n",
    "直观地解释，就是我们将这个模型文件保存到了电脑里，我们的问题都是会经过这个文件进行处理，而不用上传到网络。\n",
    "\n",
    "唯一的问题就是让开源大模型做回答，对于自己的电脑的配置要求比较高。显卡内存必须超过8G才能流畅运行目前10亿参数以下的最强模型类似glm4,谷歌的gemma2等。\n",
    "\n",
    "目前最强的开源大模型Qwen-72B足以比肩国内最强AI比如deepseek,智谱等，但是这么大的模型，普通的消费级笔记本电脑是运行不了的。\n",
    "如果有服务器，可以将这个大模型部署到自己的服务器中使用这个模型来分析敏感数据。\n",
    "这个大模型足够聪明，能够胜任非常多复杂的任务。\n",
    "\n",
    "能在自己电脑上运行的大模型只有10B以下的**qwen2,internlm2,gemma2,glm4**等，这些模型事实上已经足够强大。\n",
    "\n",
    "我们可以参考前几期的框架，构建一个用本地大模型分析论文的智能体，让大模型帮我们总结论文、给论文打分以及改论文等等。\n",
    "分别对应代码中的summarize、rating和polish，效果如图所示。\n",
    "\n",
    "需要注意的是，如果论文字数过长，这几个模型可能表现不会特别好，因此建议限制一下输入字数。\n",
    "仅有qwen2和glm4支持128K的上下文。\n",
    "\n",
    "agent-101项目是个人维护的项目，近期的智能体较为复杂，有一些文件的资料在逐渐上传中~欢迎持续关注\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![大模型排行-2407](images/1720751759154.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ollama 开源大模型分析论文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from functools import wraps  \n",
    "from super_agent_tools.code_interpreter import *\n",
    "from super_agent_tools.json_tool import *\n",
    "from super_agent_tools.pdf_reader import *\n",
    "\n",
    "class OllamaEssayAgent:\n",
    "    \"\"\"\n",
    "    目标：使用开源大模型分析论文，包括总结、打分、优化\n",
    "    优点：免费，支持中文，不存在机密信息泄露风险\n",
    "    缺陷：对电脑配置要求高，模型能力不如最强GPT-4o(但是已经足够聪明)\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.system_prompt = \"你必须尽可能详细回答我的问题。\"\n",
    "        self.conversations = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        # 总结论文的提示词\n",
    "        self.summarize_prompt = self.get_prompt(\"prompts/ollama_essay_agent/summarize_prompt.txt\")\n",
    "        # 论文打分的提示词\n",
    "        self.rating_prompt = self.get_prompt(\"prompts/ollama_essay_agent/rating_prompt.txt\")\n",
    "        # 论文优化的提示词\n",
    "        self.polish_prompt = self.get_prompt(\"prompts/ollama_essay_agent/polish_prompt.txt\")\n",
    "        \n",
    "    def get_prompt(self, path):\n",
    "        with open(path, 'r', encoding=\"utf-8\") as file:\n",
    "            prompt = file.read()\n",
    "        return prompt\n",
    "\n",
    "    def get_ollama_yield(self, messages, model):\n",
    "        client = OpenAI(\n",
    "            base_url = 'http://localhost:11434/v1',\n",
    "            api_key='ollama', \n",
    "        )\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "        )\n",
    "        for chunk in response:\n",
    "            yield chunk.choices[0].delta.content\n",
    "\n",
    "    def get_pdf_text(self, pdf_path):\n",
    "        \"\"\"\n",
    "        从PDF文件中提取文本\n",
    "        \"\"\"\n",
    "        return extract_clean_text_from_pdf(pdf_path, method='pdfminer')\n",
    "    \n",
    "    # 分析论文：总结、打分、优化\n",
    "    def analyze_essay(self, pdf_path, limit=4e4, analyze_type=\"summarize\"):\n",
    "        text = self.get_pdf_text(pdf_path)[:int(limit)]\n",
    "        print(f\"PDF提取字数:{len(text)}\")\n",
    "        # 判断论文分析的类型\n",
    "        if analyze_type == \"summarize\":\n",
    "            prompt = self.summarize_prompt.format(text=text)\n",
    "        elif analyze_type == \"rating\":\n",
    "            prompt = self.rating_prompt.format(text=text)\n",
    "        elif analyze_type == \"polish\":\n",
    "            prompt = self.polish_prompt.format(text=text)\n",
    "            \n",
    "        conversations = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        conversations.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        ans = \"\"\n",
    "        for char in self.get_ollama_yield(conversations, self.model):\n",
    "            print(char, end=\"\", flush=True)\n",
    "            ans += char\n",
    "        print()\n",
    "        return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以！为了帮助理解，我们从头开始梳理一个Transformer模型中的每一层功能。\n",
      "\n",
      "### 1. **嵌入层（Embedding Layer）**\n",
      "\n",
      "- **作用**：将输入数据转换为可以供神经网络处理的形式，通常是文本序列的情况下，将单词映射到高维空间的向量。\n",
      "- **原理**：\n",
      "  - 对每个单词进行分词得到相应的标记或索引（如在BERT模型中使用的词表）。\n",
      "  - 将这些索引转换为其对应的嵌入向量。嵌入层通常会有一个权重矩阵，该矩阵决定了每个索引与高维空间中对应位置的向量。\n",
      "\n",
      "### 2. **位置编码（Positional Encoding Layer）**\n",
      "\n",
      "- **作用**：帮助模型理解输入序列中单词的位置信息。\n",
      "- **原理**：\n",
      "  - 对于每一个位置i，生成一个特定的位置编码向量。这个编码可以包含时间信息和相对距离信息。\n",
      "  - 常见的是使用数学函数生成，例如sin和cos函数应用在不同频率的指数上，这样可以捕获序列中元素的时间顺序信息。\n",
      "\n",
      "### 3. **自注意力机制（Self-Attention Layer）**\n",
      "\n",
      "- **作用**：允许模型考虑输入序列中的每个词与所有其他词之间的相关性。\n",
      "- **原理**：\n",
      "  - 输入由一个查询(Q)，键(K)和值(V)组成，分别代表对输入序列中每个单词的特征提取。Q、K、V都从原始输入映射到不同的向量空间中。\n",
      "  - 模型计算每一对词之间的相似性（注意力权重）并通过这些权重加权平均值V得到新的输出表示。公式为：\\[Attention(Q,K,V) = softmax(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}) \\cdot V\\]，其中\\(d_k\\)是键的维度。\n",
      "  - 这个过程允许模型聚焦于序列中的不同部分，这对于处理自然语言任务非常有效。\n",
      "\n",
      "### 4. **多头自注意力（Multi-Head Attention）**\n",
      "\n",
      "- **作用**：通过并行计算多个注意力机制来提高模型的表达能力。\n",
      "- **原理**：\n",
      "  - 将输入分割成多个较小的子空间（头部），每个子空间内使用一个独立的自注意力机制进行处理。这允许模型从不同的“关注视角”捕获信息，增强对上下文的理解和任务泛化能力。\n",
      "\n",
      "### 5. **前馈神经网络（Feed-Forward Neural Network）**\n",
      "\n",
      "- **作用**：提供非线性变换以丰富经过多头自注意力层处理的特征表示。\n",
      "- **原理**：\n",
      "  - 包括两部分：一个线性转换层将输入映射到一个较大的维度空间，然后通过激活函数如ReLU进行非线性变换，最后再通过另一个线性转换层降低到原来的输出大小。\n",
      "\n",
      "### 6. **残差连接（Residual Connections）**\n",
      "\n",
      "- **作用**：在每一层之后添加输入作为额外的“残留”信息，帮助训练深层网络时保持梯度稳定。\n",
      "- **原理**：\n",
      "  - 通常在多头自注意力或前馈神经网络之后，将输入和输出结果相加，再通过一个层归一化（Layer Normalization）处理以减少内部方差。\n",
      "\n",
      "### 7. **层归一化（Layer Normalization）**\n",
      "\n",
      "- **作用**：稳定训练过程并加速收敛。\n",
      "- **原理**：\n",
      "  - 计算每个样本在每一层的平均和方差，然后将每个样本映射到标准正态分布上。这有助于减少梯度消失或爆炸问题。\n",
      "\n",
      "### 综合起来\n",
      "\n",
      "这些组成部分组合在一起构成了一个强大的Transformer模型，它可以在多个自然语言处理任务中实现卓越性能。每一个部分都为后续处理提供了关键信息，从词的表示、位置理解、注意力机制到非线性变换，以及在每一层之间的信息流动，使得Transformer能够处理复杂和长距离依赖关系的任务。"
     ]
    }
   ],
   "source": [
    "# ollama聊天测试\n",
    "question = \"通俗易懂地解释Transformer模型每一层的原理。\"\n",
    "\n",
    "model = \"gemma2\"\n",
    "model = \"internlm2\"\n",
    "model = \"codegeex4\"\n",
    "model = \"glm4\" # 128K\n",
    "model = \"qwen2\" # 128K\n",
    "\n",
    "ollama_agent = OllamaEssayAgent(model)\n",
    "ollama_agent.conversations.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "ans = \"\"\n",
    "for char in ollama_agent.get_ollama_yield(ollama_agent.conversations, model):\n",
    "    print(char, end=\"\", flush=True)\n",
    "    ans += char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF提取字数:40000\n",
      "本文主要介绍了在预训练语言模型（Language Model, LM）的基础上，引入搜索算法来改善其在现实世界的web任务中的表现。文章的结构可以概括为：\n",
      "\n",
      "**1. 预训练语言模型面临的挑战**\n",
      "\n",
      "首先，文中阐述了利用语言模型作为基础进行自动推理和决策时遇到的主要问题：采样不确定性。当从文本分布中随机抽样时生成动作，首次产生的行动可能并非在环境中最佳的行动。此外，在实时、复杂的web环境中，预训练的语言模型有时会面临失败情况，尤其是无法正确理解网页布局或用户意图等。\n",
      "\n",
      "**2. 介绍搜索策略**\n",
      "\n",
      "为了克服上述挑战，作者引入了一种基于搜索的最佳优先树搜索算法。此方法通过构建多条可能的动作轨迹来扩展行动空间，并评估每条轨迹的潜在结果。具体来说：\n",
      "\n",
      "- **Best-First Tree Search**: 在每个动作决策点上，利用预训练的语言模型（LM）预测不同选项的结果，并采用深度优先策略探索可能的路径。\n",
      "  \n",
      "- **Feedback Integration**: 实行每一步后，根据环境反馈来调整和优化未来步骤的预测。\n",
      "\n",
      "- **Trajectory Evaluation**: 利用模拟器在虚拟环境中执行这些轨迹评估，以选择最有效的路径实现目标。\n",
      "\n",
      "**3. 论文的主要发现**\n",
      "\n",
      "该方法在多个VisWebArena基准测试中显著提升了LM代理的表现：\n",
      "\n",
      "- **高成功率**: 搜索策略能帮助LM代理克服初次生成的不理想行动，并通过探索找到更高效的执行顺序来完成任务。\n",
      "  \n",
      "- **性能提升**: 与未采用搜索的基线相比，具有搜索能力的LM模型在多个复杂web任务中的成功率大幅提升。\n",
      "\n",
      "**4. 实现限制**\n",
      "\n",
      "文章也探讨了实现中可能遇到的问题：\n",
      "\n",
      "- **时间延迟**: 搜索过程需要更多推理计算时间来评估不同的动作路径，从而增加了任务完成的整体时间。\n",
      "  \n",
      "- **不可逆操作风险**: 某些网页操作（如在电子商务网站上提交订单）可能会改变页面状态，并且难以恢复。\n",
      "\n",
      "**结论**\n",
      "\n",
      "总而言之，本文提出了一个有效的方法，在预训练的语言模型代理执行复杂web任务时引入搜索策略，以此显著提升其性能并提高完成任务的成功率。该研究为构建能够自主进行规划、推理和实际操作的智能代理奠定了基础。尽管面临一些实现限制，但作者强调了在未来工作中利用更高效算法和技术解决这些问题的重要性，以便在现实世界中部署此类智能体。\n",
      "==================================================\n",
      "PDF提取字数:40000\n",
      "### 论文摘要\n",
      "\n",
      "本文提出了一种基于搜索的方法来增强语言模型（LM）代理的性能，使其在现实世界网页环境中更有效地完成任务。主要贡献如下：\n",
      "\n",
      "1. **方法简介**：引入了将最佳优先树搜索与预先训练的语言模型结合使用的策略，这允许代理探索和评估多个动作轨迹，以提高完成任务的成功率。\n",
      "\n",
      "2. **应用范围**：这种方法适用于现实世界的web任务，尤其是那些要求自动化执行复杂的网页操作的任务。例如，在VisualWebArena（VWA）基准上进行了验证，VWA是一个针对图像搜索、网页表单填写、网页导航和在线交易等任务的评估平台。\n",
      "\n",
      "3. **实验结果**：与常规语言模型代理相比，集成搜索后在完成任务时取得显著更高的成功率，并且显示了搜索算法能够优化决策的过程。\n",
      "\n",
      "4. **局限性和考虑因素**：尽管这种方法取得了积极进展，但实际部署时存在一些限制。例如，搜索可能导致额外的计算开销和环境交互时间增加，这取决于具体的搜索参数设置（如节点扩张数）。此外，在现实世界应用中可能需要对破坏性动作进行控制。\n",
      "\n",
      "### 优点\n",
      "\n",
      "1. **提升性能**：通过在决策过程中引入探索，显著提高了任务完成的成功率。\n",
      "2. **适应复杂任务**：特别针对那些要求理解网页结构、导航和交互的复杂操作场景非常有效。\n",
      "3. **灵活性**：该方法较为通用，可应用于多种类型的web任务。\n",
      "\n",
      "### 缺点\n",
      "\n",
      "1. **计算资源需求高**：搜索过程增加了额外的计算负载，特别是在需要进行大量节点扩展时。\n",
      "2. **环境与任务限制**：在某些情况下（如涉及不可逆操作的web环境），需要对动作选择进行更多的人工约束或改进算法以避免破坏性行为。\n",
      "\n",
      "### 总体评价\n",
      "\n",
      "本文通过实证研究展示了将搜索技术整合到语言模型代理中的方法是有效的，尤其是在处理具有挑战性的现实世界web任务时。尽管存在一些局限性和考虑因素（如计算效率和环境限制），但该方法提供了提升性能、适应复杂多变场景的途径。在评估中可以给论文打分为8.5/10分，考虑到其创新性、实用性与潜在应用价值，但仍存在一定的优化空间，尤其是在降低计算成本和增强通用性方面。\n"
     ]
    }
   ],
   "source": [
    "# 测试论文文件\n",
    "pdf_path = r'E:\\SuperAgent\\智能体2.0开发-深度\\files\\TREE SEARCH FOR LANGUAGE MODEL AGENTS.pdf'\n",
    "\n",
    "model = \"gemma2\"\n",
    "model = \"internlm2\"\n",
    "model = \"glm4\"\n",
    "model = \"qwen2\"\n",
    "\n",
    "ollama_agent = OllamaEssayAgent(model)\n",
    "# 总结论文\n",
    "summary = ollama_agent.analyze_essay(pdf_path, analyze_type=\"summarize\")\n",
    "print(\"=\"*50)\n",
    "# 为论文评分\n",
    "rating = ollama_agent.analyze_essay(pdf_path, analyze_type=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF提取字数:40000\n",
      "论文《Inference-time Search Enhances Language Model Agents for Web Tasks》在提出一种新的搜索算法以提高语言模型代理在实际网页任务中的能力时，确实有其创新和亮点，并且为当前领域提供了一种有前景的方法。然而，在评价其合理性以及潜在优化空间时，我们可能可以从几个角度进行考量：\n",
      "\n",
      "### 1. 方法论与实验设计的详细度\n",
      "- **优化建议**：\n",
      "   - 增加更多定量和定性的方法比较：可以添加更多的基线模型对比，比如引入无搜索、只依赖LM预测的结果进行比较。这能够更直观地展示所提方法的优势。\n",
      "   - 对于评估指标的解释和选择应更加详细。例如，如果使用了成功完成任务的比例、平均执行时间等作为评估指标，请在论文中详细阐述为什么选择了这些指标，以及它们如何反映模型的有效性和效率。\n",
      "\n",
      "### 2. 实验场景与数据集的局限性\n",
      "- **优化建议**：\n",
      "   - 对于使用的数据集（如VisualWebArena），应进一步讨论其适用范围和可能的局限性。特别是当数据集规模、类型或复杂度在现实世界中有较大差异时，如何迁移所提方法的有效性和效能。\n",
      "   - 需要更多地探讨数据标注的质量和一致性问题，并评估这些问题对最终结果的影响。\n",
      "\n",
      "### 3. 模型选择与搜索策略的优化\n",
      "- **优化建议**：\n",
      "   - 调整或引入更先进的LM模型。在现有的研究中，通常会使用GPT等模型进行实验。考虑更多地探索其他类型的语言模型（如Transformer、BERT等），以评估其对搜索算法性能的影响。\n",
      "   - 研究不同的搜索策略的组合和效果，比如深度优先搜索、广度优先搜索、A*搜索等是否能够提高任务执行效率或成功率。\n",
      "\n",
      "### 4. 搜索过程中的细节优化\n",
      "- **优化建议**：\n",
      "   - 详细讨论并对比了在搜索过程中参数（如c=20）的选择对结果的影响。可以提供更广泛的参数探索，或者通过元学习方法自动调整这些参数。\n",
      "   - 对于如何减少搜索带来的额外计算开销和时间成本进行更深入的研究。比如，利用缓存机制存储部分搜索路径的结果，或在模型训练阶段优化相关策略以减轻运行时的负担。\n",
      "\n",
      "### 5. 可持续性和泛化能力\n",
      "- **优化建议**：\n",
      "   - 讨论方法在长时间执行和大规模部署中的可持续性问题，包括对能源消耗、环境影响等方面的考虑。这不仅限于当前的数据集或特定任务，还应考虑到未来可能引入的新挑战。\n",
      "   - 探讨如何增强模型的泛化能力，使其能够适应各种不同的网页平台、格式变化以及用户的个性化需求。\n",
      "\n",
      "### 6. 遵循伦理与社会责任\n",
      "- **优化建议**：\n",
      "   - 论文应该详细说明并讨论算法对用户隐私的影响。特别是在处理网页内容和执行可能涉及敏感数据的操作时。\n",
      "   - 考虑模型的可解释性问题，尤其是搜索决策过程中。这不仅有助于增加用户的信任度，也便于监管机构评估其潜在影响。\n",
      "\n",
      "通过这些优化建议，论文可以提供更为全面、深入且实用的研究成果，并增强其对实际应用和未来研究的指导价值。\n"
     ]
    }
   ],
   "source": [
    "# 优化论文\n",
    "polish = ollama_agent.analyze_essay(pdf_path, analyze_type=\"polish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
