{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 具有知识库配置和索引功能的Agent\n",
    "\n",
    "做一个知识库助手非常有意思且有用，如果做得好，是可以让这个助手精准回答任何关于知识库的问题的。\n",
    "这里主要涉及到的知识就是RAG(检索增强生成)。\n",
    "一个复杂的高效的RAG包含非常多模块，但是我们可以从一个简单的做起。\n",
    "\n",
    "简单的，能用的RAG，涉及到以下几个模块：\n",
    "数据索引：将知识库进行清洗，构建RAG的索引数据，也就是将数据库分块保存到数据库里\n",
    "检索：找到与用户问题最相关的数据\n",
    "生成：将找到的数据发给大模型生成回答\n",
    "\n",
    "在我这个简单的智能体里边，我用以下两个函数提取PDF文本：\n",
    "- text = self.extract_text_from_pdf(pdf_path)\n",
    "- paragraphs = self.split_text_into_paragraphs(text)\n",
    "PDF的文本会被放进一个列表里，然后调用智谱的嵌入模型将这个列表里的每个文本都转换为向量保存起来。（可以放进excel也可以放入数据库）\n",
    "然后用get_pdf_relevant_paragraphs这个函数找到与用户的问题最相关的两个段落，发给大模型，让大模型根据这两个段落回答用户的问题。\n",
    "\n",
    "这里我还增加了一个算法就是识别用户的问题是否与知识库相关，如果不相关则不做处理并提醒用户，但是其实可以在不相关的时候直接让大模型回复用户就行了~\n",
    "这就留给各位进行发挥~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.code_interpreter import *\n",
    "from tools.json_tool import *\n",
    "from tools.llm_api import *\n",
    "from tools.llm_keys import *\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "\n",
    "class RAGAgent:\n",
    "    def __init__(self, model, zhipu_key, print_ans=True):\n",
    "        self.model = model\n",
    "        self.temperature = 0.9\n",
    "        self.api_key = zhipu_key\n",
    "        self.print_ans = print_ans\n",
    "        self.judge_cmd_prompt = self.get_prompt(\"RAG/judge_cmd_prompt.txt\")\n",
    "        self.answer_questions_prompt = self.get_prompt(\"RAG/answer_questions_prompt.txt\")\n",
    "        \n",
    "        # 获取RAG/files文件夹下面的所有文件名称\n",
    "        self.files = os.listdir(\"RAG/files\")\n",
    "        self.files = [file for file in self.files if file.endswith(\".pdf\")]\n",
    "        \n",
    "        self.system_prompt = '''\n",
    "你是我的知识库助理，拥有大量的知识库的信息，你将根据这些信息回答我的问题。\n",
    "\n",
    "# 当前拥有的知识库文件\n",
    "{files}\n",
    "'''\n",
    "        self.system_prompt = self.system_prompt.format(files=self.files)\n",
    "        self.conversations = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        \n",
    "    def get_prompt(self, path):\n",
    "        with open(path, 'r', encoding=\"utf-8\") as file:\n",
    "            prompt = file.read()\n",
    "        return prompt\n",
    "\n",
    "    def get_llm_ans_v1(self, conversations):\n",
    "        if self.print_ans:\n",
    "            ans = \"\"\n",
    "            for char in get_llm_answer_converse(conversations, self.model, self.temperature):\n",
    "                ans += char\n",
    "                print(char, end=\"\", flush=True)\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            ans = \"\"\n",
    "            for char in get_llm_answer(conversations, self.model, self.temperature):\n",
    "                ans += char\n",
    "        return ans\n",
    "    \n",
    "    def get_llm_ans_converse(self, question):\n",
    "        self.conversations.append({\"role\": \"user\", \"content\": question})\n",
    "        if self.print_ans:\n",
    "            ans = \"\"\n",
    "            for char in get_llm_answer_converse(self.conversations, self.model, self.temperature):\n",
    "                ans += char\n",
    "                print(char, end=\"\", flush=True)\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            ans = \"\"\n",
    "            for char in get_llm_answer_converse(self.conversations, self.model, self.temperature):\n",
    "                ans += char\n",
    "        return ans\n",
    "    \n",
    "    # 提取PDF文本\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "        return self.clean_text(text)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # 移除多余的空白字符\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # 移除特殊字符，但保留标点符号\n",
    "        text = ''.join([char for char in text if char.isalnum() or char.isspace() or char in string.punctuation])\n",
    "        \n",
    "        # 统一标点符号的格式（例如，将中文引号转换为英文引号）\n",
    "        text = text.replace('\"', '\"').replace('\"', '\"')\n",
    "        text = text.replace(''', \"'\").replace(''', \"'\")\n",
    "        \n",
    "        # 确保句子之间有正确的空格\n",
    "        text = re.sub(r'([.!?。！？])\\s*', r'\\1 ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    # 文本分段（小于1000字）\n",
    "    def split_text_into_paragraphs(self, text, max_length=2000):\n",
    "        # 使用正则表达式查找句子结束的位置（句号、问号或感叹号）\n",
    "        sentences = re.split(r'(?<=[.?!])\\s*', text)\n",
    "        paragraphs = []\n",
    "        current_paragraph = \"\"\n",
    "        for sentence in sentences:\n",
    "            # 检查加上当前句子后的长度是否超过限定的最大长度\n",
    "            if len(current_paragraph + sentence) <= max_length:\n",
    "                current_paragraph += sentence\n",
    "            else:\n",
    "                # 如果当前段落加上这句话会超过1000字，先保存当前段落，然后新起一个段落\n",
    "                if current_paragraph:\n",
    "                    paragraphs.append(current_paragraph)\n",
    "                current_paragraph = sentence\n",
    "        # 添加最后一个段落，如果它非空\n",
    "        if current_paragraph:\n",
    "            paragraphs.append(current_paragraph)\n",
    "        return paragraphs\n",
    "\n",
    "    # GLM嵌入模型\n",
    "    def embedding(self, text):\n",
    "        client = ZhipuAI(api_key=self.api_key) \n",
    "        response = client.embeddings.create(\n",
    "            model=\"embedding-2\",\n",
    "            input=text,\n",
    "        )\n",
    "        ebd = response.data[0].embedding\n",
    "        return ebd\n",
    "    \n",
    "    # 用户问题向量化\n",
    "    def query_embedding(self, query):\n",
    "        return self.embedding(query)\n",
    "\n",
    "    # 计算两个向量之间的相似性\n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        norm_vec1 = np.linalg.norm(vec1)\n",
    "        norm_vec2 = np.linalg.norm(vec2)\n",
    "        return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "    # 计算问题向量与每个段落之间的相关性，用列表存储，降序排列\n",
    "    def compute_similarities(self, paragraph_vectors, question_vector):\n",
    "        similarities = []\n",
    "        for index, pvec in enumerate(paragraph_vectors):\n",
    "            sim = self.cosine_similarity(pvec, question_vector)\n",
    "            similarities.append((index, sim))\n",
    "        \n",
    "        # 按相似度降序排序\n",
    "        sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "        return sorted_similarities\n",
    "\n",
    "    # 找出pdf中与用户问题最相关的两个段落\n",
    "    def get_pdf_relevant_paragraphs(self, query, paragraphs, pdf_ebd):\n",
    "        # 量化用户问题\n",
    "        query_ebd = self.query_embedding(query)\n",
    "\n",
    "        # 找到与用户问题最相关的pdf段落\n",
    "        relevant_paragraphs_list = self.compute_similarities(pdf_ebd, query_ebd)\n",
    "        relevant_paragraphs_list = relevant_paragraphs_list[:2]\n",
    "        relevant_paragraphs = \"\"\n",
    "\n",
    "        print('最相关的两个pdf段落：')\n",
    "        for i in relevant_paragraphs_list:\n",
    "            relevant_paragraphs += paragraphs[i[0]]\n",
    "            print(f\"paragrah{i[0]}:\\n\", paragraphs[i[0]])\n",
    "            \n",
    "        # 返回段落\n",
    "        return relevant_paragraphs\n",
    "\n",
    "    # 判断问题是否与知识库相关\n",
    "    def judge_cmd(self, question):\n",
    "        prompt = self.judge_cmd_prompt\n",
    "        prompt = prompt.format(question=question)\n",
    "        conversation_temp = self.conversations.copy()\n",
    "        conversation_temp.append({\"role\": \"user\", \"content\": prompt})\n",
    "        ans = self.get_llm_ans_v1(conversation_temp)\n",
    "        ans = get_json(ans)\n",
    "        is_related = ans[\"is_related\"]\n",
    "        print(is_related)\n",
    "        return is_related\n",
    "    \n",
    "    def answer_questions(self, question):\n",
    "        # 判断问题是否与知识库相关\n",
    "        is_related = self.judge_cmd(question)\n",
    "        if not is_related:\n",
    "            return \"这个问题不是关于知识库的问题，我无法回答。\"\n",
    "\n",
    "        # 提取pdf文本\n",
    "        pdf_path = \"RAG/files/Adaptive In-conversation Team Building.pdf\"\n",
    "        print(f\"正在处理pdf文件：{pdf_path}\")\n",
    "        text = self.extract_text_from_pdf(pdf_path)\n",
    "        paragraphs = self.split_text_into_paragraphs(text)\n",
    "\n",
    "        # 获取pdf段落的向量表示\n",
    "        print(\"正在计算pdf段落的向量表示...\")\n",
    "        pdf_ebd = [self.embedding(paragraph) for paragraph in paragraphs]\n",
    "\n",
    "        # 找出pdf中与用户问题最相关的两个段落\n",
    "        relevant_paragraphs = self.get_pdf_relevant_paragraphs(question, paragraphs, pdf_ebd)\n",
    "\n",
    "        prompt = self.answer_questions_prompt\n",
    "        prompt = prompt.format(question=question, relevant_paragraphs=relevant_paragraphs)\n",
    "        ans = self.get_llm_ans_converse(prompt)\n",
    "        self.conversations.append({\"role\": \"assistant\", \"content\": ans})\n",
    "        return ans\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"glm-4\"\n",
    "knowledge_agent = RAGAgent(model, zhipu_key)\n",
    "\n",
    "question = \"我知识库中的论文的主要内容是什么？\"\n",
    "answer = knowledge_agent.answer_questions(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
